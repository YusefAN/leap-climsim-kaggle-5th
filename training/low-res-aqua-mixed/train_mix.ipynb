{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28fa8305-e356-4a3a-b5dc-39c3ff04ed7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchmetrics.regression import R2Score\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import hickle as hkl\n",
    "from torch.nn import AvgPool1d \n",
    "import torch.nn as nn\n",
    "import gc\n",
    "from glob import glob\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "\n",
    "DATA_PATH = 'autodl-tmp/'\n",
    "\n",
    "\n",
    "file_list = glob(DATA_PATH+'CDFrag/'+'c_data_frag_x*hkl')\n",
    "\n",
    "file_list_mix = glob(DATA_PATH+'CDFrag L/'+'c_data_frag_x_*_1e-15.hkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1993d7a8-4b52-46c3-b612-d5f76bfa2c3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34296"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f8f6dcd-e762-4dc2-ad6a-d2938db5ddf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96672"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(file_list_mix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1605d537-3e9d-4b17-9ec0-861bace0e118",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51685c0b-ec42-405b-ab69-fe76fe7af8e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'autodl-tmp/CDFrag/c_data_frag_x_1_2_0_1e-20.hkl'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fdf2ba5-2f6c-43f9-8311-67d790adfa73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'autodl-tmp/CDFrag L/c_data_frag_x_7_1_0_1e-15.hkl'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_list_mix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbf70364-97af-442f-ade2-2b934931f554",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len('autodl-tmp/CDFrag/c_data_frag_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cc69572-b58f-425f-adbb-1732fd5c97b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len('autodl-tmp/CDFrag L/c_data_frag_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87633ff0-d449-4ac4-b812-de9d5315bbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "zeroout_index = list(range(145,147))\n",
    "\n",
    "zeroout_index = torch.tensor(list(set(zeroout_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd28dcf4-9688-4d2d-866c-5a910502998e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import polars as pl\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchmetrics.regression import R2Score\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import hickle as hkl\n",
    "from torch.nn import AvgPool1d \n",
    "import torch.nn as nn\n",
    "import gc\n",
    "import glob\n",
    "import random\n",
    "import math\n",
    "\n",
    "from torch.nn import LSTM, Conv1d, GRU, TransformerEncoder, TransformerEncoderLayer, BatchNorm1d\n",
    "\n",
    "\n",
    "from torch.nn import LayerNorm\n",
    "\n",
    "from matplotlib.pyplot import plot\n",
    "# tmp = addition_y[10:774144:384,10]\n",
    "# tmp = addition_y[:,2]\n",
    "\n",
    "\n",
    "BATCH_SIZE = 384*6*3\n",
    "MIN_STD = 1e-10\n",
    "SCHEDULER_PATIENCE = 6\n",
    "SCHEDULER_FACTOR = 10**(-0.2)\n",
    "EPOCHS = 70\n",
    "PATIENCE = 6\n",
    "PRINT_FREQ = 300\n",
    "BIN_NUM = 10\n",
    "n_rounds = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32c6034-e310-4b9b-8101-bca328d2f7bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "706c3681-81f4-4efe-a3d0-8c8b638827f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "seq_fea_list = ['state_t','state_q0001','state_q0002','state_q0003','state_u','state_v','pbuf_ozone','pbuf_CH4','pbuf_N2O']\n",
    "num_fea_list = ['state_ps','pbuf_SOLIN','pbuf_LHFLX','pbuf_SHFLX','pbuf_TAUX','pbuf_TAUY','pbuf_COSZRS','cam_in_ALDIF','cam_in_ALDIR','cam_in_ASDIF','cam_in_ASDIR','cam_in_LWUP','cam_in_ICEFRAC','cam_in_LANDFRAC','cam_in_OCNFRAC','cam_in_SNOWHLAND']\n",
    "\n",
    "\n",
    "seq_y_list = ['ptend_t','ptend_q0001','ptend_q0002','ptend_q0003','ptend_u','ptend_v']\n",
    "num_y_list = ['cam_out_NETSW','cam_out_FLWDS','cam_out_PRECSC','cam_out_PRECC','cam_out_SOLS','cam_out_SOLL','cam_out_SOLSD','cam_out_SOLLD']\n",
    "\n",
    "seq_fea_expand_list = []\n",
    "for i in seq_fea_list:\n",
    "    for j in range(60):\n",
    "        seq_fea_expand_list.append(i+'_'+str(j))\n",
    "\n",
    "\n",
    "seq_y_expand_list = []\n",
    "for i in seq_y_list:\n",
    "    for j in range(60):\n",
    "        seq_y_expand_list.append(i+'_'+str(j))\n",
    "        \n",
    "norm_dict = dict()\n",
    "TARGET_COLS = seq_y_expand_list + num_y_list\n",
    "FEAT_COLS = seq_fea_expand_list + num_fea_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c3151eb-5315-4ce2-9257-e9ed6f912b5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "556"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(FEAT_COLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a006ce38-a8de-4004-a537-b07293bf4701",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FFNN_LSTM_6_AVG_PLE(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(FFNN_LSTM_6_AVG_PLE, self).__init__()\n",
    "        \n",
    "        self.encode_dim = 280\n",
    "        self.hidden_dim = 260\n",
    "        self.iter_dim = 1024\n",
    "        self.num_emb = 8\n",
    "        \n",
    "        self.PiecewiseLinearEmbeddings = PiecewiseLinearEmbeddings(bins, self.num_emb-1, activation=False)\n",
    "\n",
    "        self.LSTM_1 = LSTM(self.encode_dim,self.hidden_dim,6,batch_first=True,dropout=0.05,bidirectional=True)\n",
    "        self.input_size = input_size\n",
    "        self.Linear_1 = nn.Linear(self.num_emb*len(seq_fea_list)+self.num_emb*len(num_fea_list), self.encode_dim)\n",
    "        self.Linear_2 = nn.Linear(6*self.hidden_dim+self.encode_dim, self.iter_dim)\n",
    "        self.Linear_3 = nn.Linear(self.iter_dim, len(seq_y_list))\n",
    "        self.Linear_4_0 = nn.Linear(self.iter_dim+self.num_emb*len(num_fea_list), self.iter_dim*2)\n",
    "\n",
    "        self.Linear_4 = nn.Linear(self.iter_dim*2, len(num_y_list))\n",
    "        self.bias = nn.Linear(len(seq_y_list)*60+len(num_y_list),1)\n",
    "        self.weight = nn.Linear(len(seq_y_list)*60+len(num_y_list),1)\n",
    "        self.avg_pool_1 = AvgPool1d(kernel_size=3,stride=1,padding=1)\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x_ple = self.PiecewiseLinearEmbeddings(x)\n",
    "        x = x[:,:,None]\n",
    "        x = torch.concat((x,x_ple),dim=-1)\n",
    "        \n",
    "         \n",
    "        # print(x_ple.shape)\n",
    "        \n",
    "        x_seq = x[:,0:60*len(seq_fea_list),:]\n",
    "        \n",
    "        x_seq = x_seq.reshape((-1,len(seq_fea_list),60,self.num_emb))\n",
    "        x_seq = torch.transpose(x_seq, 1, 2)\n",
    "        x_seq = x_seq.reshape((-1,60,len(seq_fea_list)*self.num_emb))\n",
    "        \n",
    "        x_num = x[:,60*len(seq_fea_list):x.shape[1],:]\n",
    "        x_num = x_num.reshape((-1,len(num_fea_list)*self.num_emb))\n",
    "\n",
    "        x_num_repeat = x_num[:,None,:]\n",
    "        x_num_repeat = x_num_repeat.repeat((1,60,1))\n",
    "        \n",
    "        x_seq = F.elu(self.Linear_1(torch.concat((x_seq,x_num_repeat),dim=-1)/5))\n",
    "        \n",
    "        x_seq_1,_ = self.LSTM_1(x_seq/5)\n",
    "        \n",
    "        x_seq_1_mean = torch.mean(x_seq_1,dim=1,keepdim=True)\n",
    "        x_seq_1_mean = x_seq_1_mean.repeat((1,60,1))\n",
    "\n",
    "        x_seq_1_avg_pool = self.avg_pool_1(torch.transpose(x_seq_1, 1, 2))\n",
    "        x_seq_1_avg_pool = torch.transpose(x_seq_1_avg_pool,1, 2)\n",
    "        \n",
    "        x_seq_1 = F.elu(self.Linear_2(torch.cat((x_seq_1,x_seq_1_mean,x_seq,x_seq_1_avg_pool),dim=-1)/5))\n",
    "        \n",
    "        x_seq_out = self.Linear_3(x_seq_1)\n",
    "        x_seq_out = torch.transpose(x_seq_out, 1, 2)\n",
    "        x_seq_out = x_seq_out.reshape((-1,60*len(seq_y_list)))\n",
    "        \n",
    "        x_num_out = torch.concat((torch.mean(x_seq_1,dim=1),x_num),dim=-1)\n",
    "        \n",
    "        x_num_out = F.elu(self.Linear_4_0(x_num_out))\n",
    "        x_num_out = self.Linear_4(x_num_out)\n",
    "        \n",
    "        # print(self.weight)\n",
    "        \n",
    "        return self.weight.weight*(torch.concat((x_seq_out,x_num_out),dim=-1))/3+self.bias.weight/3\n",
    "\n",
    "\n",
    "seq_fea_expand_list = []\n",
    "for i in seq_fea_list:\n",
    "    for j in range(60):\n",
    "        seq_fea_expand_list.append(i+'_'+str(j))\n",
    "\n",
    "\n",
    "seq_y_expand_list = []\n",
    "for i in seq_y_list:\n",
    "    for j in range(60):\n",
    "        seq_y_expand_list.append(i+'_'+str(j))\n",
    "        \n",
    "norm_dict = dict()\n",
    "TARGET_COLS = seq_y_expand_list + num_y_list\n",
    "FEAT_COLS = seq_fea_expand_list + num_fea_list\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class NumpyDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        \"\"\"\n",
    "        Initialize with NumPy arrays.\n",
    "        \"\"\"\n",
    "        assert x.shape[0] == y.shape[0], \"Features and labels must have the same number of samples\"\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Total number of samples.\n",
    "        \"\"\"\n",
    "        return self.x.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Generate one sample of data.\n",
    "        \"\"\"\n",
    "        # Convert the data to tensors when requested\n",
    "        return torch.from_numpy(self.x[index]).float().to(device), torch.from_numpy(self.y[index]).float().to(device)\n",
    "\n",
    "x_test = hkl.load(DATA_PATH+'x_test_v1_1e-15.hkl')\n",
    "x_valid = hkl.load(DATA_PATH+'c_data_x_8_1_v1_1e-15.hkl')\n",
    "y_valid = hkl.load(DATA_PATH+'c_data_y_8_1_v1_1e-15.hkl')\n",
    "\n",
    "\n",
    "def gen_pe(max_length, d_model, n):\n",
    "\n",
    "  # generate an empty matrix for the positional encodings (pe)\n",
    "  pe = np.zeros(max_length*d_model).reshape(max_length, d_model) \n",
    "\n",
    "  # for each position\n",
    "  for k in np.arange(max_length):\n",
    "\n",
    "    # for each dimension\n",
    "    for i in np.arange(d_model//2):\n",
    "\n",
    "      # calculate the internal value for sin and cos\n",
    "      theta = k / (n ** ((2*i)/d_model))       \n",
    "\n",
    "      # even dims: sin   \n",
    "      pe[k, 2*i] = math.sin(theta) \n",
    "\n",
    "      # odd dims: cos               \n",
    "      pe[k, 2*i+1] = math.cos(theta)\n",
    "\n",
    "  return pe\n",
    "\n",
    "\n",
    "val_dataset = NumpyDataset(x_valid, y_valid)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a79c9c8c-a979-4ad7-9cac-77ba62c3d492",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "196cdadb-66a6-4e08-9e28-66968720518c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time after processing data: 0:00:00\n",
      "Time after all preparations: 0:00:01\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def format_time(elapsed):\n",
    "    \"\"\"Take a time in seconds and return a string hh:mm:ss.\"\"\"\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "ts = time.time()\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "print(\"Time after processing data:\", format_time(time.time()-ts), flush=True)    \n",
    "\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "\n",
    "class FFNN_Transformer_1_Renorm(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(FFNN_Transformer_1_Renorm, self).__init__()\n",
    "        \n",
    "        self.encode_dim = 180\n",
    "        self.hidden_dim = 180\n",
    "        self.iter_dim = 1024\n",
    "        \n",
    "        self.pe = gen_pe(max_length=60, d_model= self.encode_dim, n=100)\n",
    "        self.pe = torch.from_numpy((self.pe).astype(np.float32)).to(device)[None,:,:]\n",
    "        self.encoder_layer = TransformerEncoderLayer(d_model=self.encode_dim, nhead=6,dim_feedforward=512,dropout=0, batch_first=True)\n",
    "        self.transformer_encoder = TransformerEncoder(self.encoder_layer, num_layers=5)  \n",
    "        # self.LSTM_1 = LSTM(self.encode_dim,self.hidden_dim,6,batch_first=True,dropout=0.05,bidirectional=True)\n",
    "        self.input_size = input_size\n",
    "        self.pos_emb_size = 20\n",
    "        self.pos_emb = nn.Linear(self.pos_emb_size,60)\n",
    "        self.Linear_1 = nn.Linear(self.pos_emb_size+len(seq_fea_list)+len(num_fea_list), self.encode_dim)\n",
    "        self.Linear_1_0 = nn.Linear(self.encode_dim, self.encode_dim)\n",
    "\n",
    "        self.Linear_2 = nn.Linear(3*self.hidden_dim+self.encode_dim, self.iter_dim)\n",
    "        self.Linear_2_0 = nn.Linear(self.iter_dim, self.iter_dim)\n",
    "        self.Linear_2_1 = nn.Linear(self.iter_dim, self.iter_dim)\n",
    "        self.Linear_2_2 = nn.Linear(self.iter_dim, self.iter_dim)\n",
    "\n",
    "        self.Linear_3 = nn.Linear(self.iter_dim, len(seq_y_list))\n",
    "        self.Linear_4 = nn.Linear(len(seq_fea_list)*60+len(num_fea_list)+60*self.iter_dim+len(num_fea_list), 2*self.iter_dim)\n",
    "        self.Linear_5 = nn.Linear(2*self.iter_dim,len(num_y_list))\n",
    "        self.bias = nn.Linear(len(seq_y_list)*60+len(num_y_list),1)\n",
    "        self.weight = nn.Linear(len(seq_y_list)*60+len(num_y_list),1)\n",
    "\n",
    "        self.fea_weight = nn.Linear(len(seq_fea_list)*60,1)\n",
    "        self.fea_bias = nn.Linear(len(seq_fea_list)*60,1)\n",
    "        self.bn1 = BatchNorm1d(self.iter_dim)\n",
    "        self.bn2 = BatchNorm1d(self.iter_dim)\n",
    "        self.bn3 = BatchNorm1d(self.iter_dim)\n",
    "        self.avg_pool_1 = AvgPool1d(kernel_size=3,stride=1,padding=1)\n",
    "    def forward(self, x):\n",
    "        x_seq = x[:,0:60*len(seq_fea_list)]\n",
    "        x_seq = x_seq*self.fea_weight.weight[None,:,:]+self.fea_bias.weight[None,:,:]\n",
    "        \n",
    "        x_num = x[:,60*len(seq_fea_list):x.shape[1]]\n",
    "        x_seq = x_seq.reshape((-1,len(seq_fea_list),60))\n",
    "        x_seq = torch.transpose(x_seq, 1, 2)\n",
    "        x_num_repeat = x_num.reshape((-1,1,len(num_fea_list)))\n",
    "        x_num_repeat = x_num_repeat.repeat((1,60,1))\n",
    "        \n",
    "        x_pos_emb = self.pos_emb.weight.reshape(1,60,self.pos_emb_size)\n",
    "        x_pos_emb = x_pos_emb.repeat((x.shape[0],1,1))\n",
    "        \n",
    "        x_seq = F.elu(self.Linear_1(torch.concat((x_seq,x_num_repeat,x_pos_emb),dim=-1)/3))\n",
    "        \n",
    "        \n",
    "        x_seq_1 = self.transformer_encoder(self.Linear_1_0(x_seq)/3+self.pe)\n",
    "        \n",
    "        x_seq_1_mean = torch.mean(x_seq_1,dim=1,keepdim=True)\n",
    "        x_seq_1_mean = x_seq_1_mean.repeat((1,60,1))\n",
    "\n",
    "        x_seq_1_avg_pool = self.avg_pool_1(torch.transpose(x_seq_1, 1, 2))\n",
    "        x_seq_1_avg_pool = torch.transpose(x_seq_1_avg_pool,1, 2)\n",
    "        \n",
    "        x_seq_1 = F.elu(self.Linear_2(torch.cat((x_seq_1,x_seq_1_mean,x_seq,x_seq_1_avg_pool),dim=-1)/8))\n",
    "        x_seq_1 = F.elu(self.Linear_2_0(x_seq_1)) + x_seq_1\n",
    "        # x_seq_1 = self.bn1(x_seq_1)\n",
    "        x_seq_1 = F.elu(self.Linear_2_1(x_seq_1)) + x_seq_1\n",
    "        # x_seq_1 = self.bn2(x_seq_1)\n",
    "        x_seq_1 = F.elu(self.Linear_2_2(x_seq_1)) + x_seq_1\n",
    "        # x_seq_1 = self.bn3(x_seq_1)\n",
    "        x_seq_out = self.Linear_3(x_seq_1/3)\n",
    "        x_seq_out = torch.transpose(x_seq_out, 1, 2)\n",
    "        x_seq_out = x_seq_out.reshape((-1,60*len(seq_y_list)))\n",
    "        \n",
    "        x_seq_out_1 = x_seq_1.reshape((-1,60*self.iter_dim))\n",
    "        x_num_out = F.elu(self.Linear_4(torch.concat((x_seq_out_1,x_num,x),dim=-1)/20))\n",
    "        x_num_out = self.Linear_5(x_num_out/4)\n",
    "        \n",
    "        # print(self.weight)\n",
    "        \n",
    "        return self.weight.weight*(torch.concat((x_seq_out,x_num_out),dim=-1))/3+self.bias.weight/3\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class FFNN_LSTM(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(FFNN_LSTM, self).__init__()\n",
    "        \n",
    "        self.encode_dim = 120\n",
    "        self.hidden_dim = 120\n",
    "        self.iter_dim = 512\n",
    "\n",
    "        \n",
    "        self.LSTM_1 = LSTM(self.encode_dim,self.hidden_dim,2,batch_first=True,dropout=0.05,bidirectional=True)\n",
    "        self.input_size = input_size\n",
    "        self.Linear_1 = nn.Linear(len(seq_fea_list)+len(num_fea_list), self.encode_dim)\n",
    "        self.Linear_2 = nn.Linear(2*self.hidden_dim+self.encode_dim, self.iter_dim)\n",
    "        self.Linear_3 = nn.Linear(self.iter_dim, len(seq_y_list))\n",
    "        self.Linear_4 = nn.Linear(60*self.iter_dim+len(num_fea_list), 2*self.iter_dim)\n",
    "        self.Linear_5 = nn.Linear(2*self.iter_dim,len(num_y_list))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_seq = x[:,0:60*len(seq_fea_list)]\n",
    "        x_num = x[:,60*len(seq_fea_list):x.shape[1]]\n",
    "        x_seq = x_seq.reshape((-1,len(seq_fea_list),60))\n",
    "        x_seq = torch.transpose(x_seq, 1, 2)\n",
    "        x_num_repeat = x_num.reshape((-1,1,len(num_fea_list)))\n",
    "        x_num_repeat = x_num_repeat.repeat((1,60,1))\n",
    "        \n",
    "        x_seq = F.elu(self.Linear_1(torch.concat((x_seq,x_num_repeat),dim=-1)/20))\n",
    "        \n",
    "        x_seq_1,_ = self.LSTM_1(x_seq/20)\n",
    "        x_seq_1 = F.elu(self.Linear_2(torch.cat((x_seq_1,x_seq),dim=-1)/20))\n",
    "        \n",
    "        x_seq_out = self.Linear_3(x_seq_1)\n",
    "        x_seq_out = torch.transpose(x_seq_out, 1, 2)\n",
    "        x_seq_out = x_seq_out.reshape((-1,60*len(seq_y_list)))\n",
    "        \n",
    "        x_seq_out_1 = x_seq_1.reshape((-1,60*self.iter_dim))\n",
    "        x_num_out = F.elu(self.Linear_4(torch.concat((x_seq_out_1,x_num),dim=-1)/20))\n",
    "        x_num_out = self.Linear_5(x_num_out/20)\n",
    "        \n",
    "        return torch.concat((x_seq_out,x_num_out),dim=-1)\n",
    "\n",
    "class FFNN_LSTM_7_AVG_ATT(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(FFNN_LSTM_7_AVG_ATT, self).__init__()\n",
    "        \n",
    "        self.encode_dim = 256\n",
    "        self.hidden_dim = 280\n",
    "        self.iter_dim = 1024\n",
    "\n",
    "        \n",
    "        self.LSTM_1 = LSTM(self.encode_dim,self.hidden_dim,6,batch_first=True,dropout=0.05,bidirectional=True)\n",
    "        self.input_size = input_size\n",
    "        self.Linear_1 = nn.Linear(len(seq_fea_list)+len(num_fea_list), self.encode_dim)\n",
    "        self.Linear_2 = nn.Linear(6*self.hidden_dim+self.encode_dim, self.iter_dim)\n",
    "        self.Linear_3 = nn.Linear(self.iter_dim, len(seq_y_list))\n",
    "        self.Linear_3_0 = nn.Linear(self.iter_dim, 1)\n",
    "\n",
    "        self.Linear_4_0 = nn.Linear(self.iter_dim, self.iter_dim*2)\n",
    "\n",
    "        self.Linear_4 = nn.Linear(self.iter_dim*2, len(num_y_list))\n",
    "        self.bias = nn.Linear(len(seq_y_list)*60+len(num_y_list),1)\n",
    "        self.weight = nn.Linear(len(seq_y_list)*60+len(num_y_list),1)\n",
    "        self.avg_pool_1 = AvgPool1d(kernel_size=3,stride=1,padding=1)\n",
    "    def forward(self, x):\n",
    "        x_seq = x[:,0:60*len(seq_fea_list)]\n",
    "        # print(x_seq)\n",
    "        x_num = x[:,60*len(seq_fea_list):x.shape[1]]\n",
    "        x_seq = x_seq.reshape((-1,len(seq_fea_list),60))\n",
    "        # print(x_seq)\n",
    "        x_seq = torch.transpose(x_seq, 1, 2)\n",
    "        x_num_repeat = x_num.reshape((-1,1,len(num_fea_list)))\n",
    "        x_num_repeat = x_num_repeat.repeat((1,60,1))\n",
    "        \n",
    "        # print(x_num_repeat.shape)\n",
    "        # print(x_seq.shape)\n",
    "\n",
    "        \n",
    "        x_seq = F.elu(self.Linear_1(torch.concat((x_seq,x_num_repeat),dim=-1)/5))\n",
    "        \n",
    "        x_seq_1,_ = self.LSTM_1(x_seq/5)\n",
    "        \n",
    "        x_seq_1_mean = torch.mean(x_seq_1,dim=1,keepdim=True)\n",
    "        x_seq_1_mean = x_seq_1_mean.repeat((1,60,1))\n",
    "\n",
    "        x_seq_1_avg_pool = self.avg_pool_1(torch.transpose(x_seq_1, 1, 2))\n",
    "        x_seq_1_avg_pool = torch.transpose(x_seq_1_avg_pool,1, 2)\n",
    "        \n",
    "        x_seq_1 = F.elu(self.Linear_2(torch.cat((x_seq_1,x_seq_1_mean,x_seq,x_seq_1_avg_pool),dim=-1)/5))\n",
    "        \n",
    "        att_weight = F.softmax(self.Linear_3_0(x_seq_1) - 10,dim=1)\n",
    "        \n",
    "        \n",
    "        x_seq_out = self.Linear_3(x_seq_1)\n",
    "        x_seq_out = torch.transpose(x_seq_out, 1, 2)\n",
    "        x_seq_out = x_seq_out.reshape((-1,60*len(seq_y_list)))\n",
    "        \n",
    "        x_num_out = F.elu(self.Linear_4_0(torch.sum(att_weight*x_seq_1,dim=1)))\n",
    "        x_num_out = self.Linear_4(x_num_out)\n",
    "        \n",
    "        # print(self.weight)\n",
    "        \n",
    "        return self.weight.weight*(torch.concat((x_seq_out,x_num_out),dim=-1))/3+self.bias.weight/3\n",
    "\n",
    "\n",
    "class FFNN_LSTM_749_AVG_ATT(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(FFNN_LSTM_749_AVG_ATT, self).__init__()\n",
    "        \n",
    "        self.encode_dim = 256\n",
    "        self.hidden_dim = 320\n",
    "        self.iter_dim = 1024\n",
    "\n",
    "        \n",
    "        self.LSTM_1 = LSTM(self.encode_dim,self.hidden_dim,6,batch_first=True,dropout=0.05,bidirectional=True)\n",
    "        self.input_size = input_size\n",
    "        self.Linear_1 = nn.Linear(len(seq_fea_list)+len(num_fea_list), self.encode_dim)\n",
    "        self.Linear_2 = nn.Linear(6*self.hidden_dim+self.encode_dim, self.iter_dim)\n",
    "        self.Linear_3 = nn.Linear(self.iter_dim, len(seq_y_list))\n",
    "        self.Linear_3_0 = nn.Linear(self.iter_dim, 1)\n",
    "\n",
    "        self.Linear_4_0 = nn.Linear(self.iter_dim, self.iter_dim*2)\n",
    "\n",
    "        self.Linear_4 = nn.Linear(self.iter_dim*2, len(num_y_list))\n",
    "        self.bias = nn.Linear(len(seq_y_list)*60+len(num_y_list),1)\n",
    "        self.weight = nn.Linear(len(seq_y_list)*60+len(num_y_list),1)\n",
    "        self.avg_pool_1 = AvgPool1d(kernel_size=3,stride=1,padding=1)\n",
    "    def forward(self, x):\n",
    "        x_seq = x[:,0:60*len(seq_fea_list)]\n",
    "        # print(x_seq)\n",
    "        x_num = x[:,60*len(seq_fea_list):x.shape[1]]\n",
    "        x_seq = x_seq.reshape((-1,len(seq_fea_list),60))\n",
    "        # print(x_seq)\n",
    "        x_seq = torch.transpose(x_seq, 1, 2)\n",
    "        x_num_repeat = x_num.reshape((-1,1,len(num_fea_list)))\n",
    "        x_num_repeat = x_num_repeat.repeat((1,60,1))\n",
    "        \n",
    "        # print(x_num_repeat.shape)\n",
    "        # print(x_seq.shape)\n",
    "\n",
    "        \n",
    "        x_seq = F.elu(self.Linear_1(torch.concat((x_seq,x_num_repeat),dim=-1)/5))\n",
    "        \n",
    "        x_seq_1,_ = self.LSTM_1(x_seq/5)\n",
    "        \n",
    "        x_seq_1_mean = torch.mean(x_seq_1,dim=1,keepdim=True)\n",
    "        x_seq_1_mean = x_seq_1_mean.repeat((1,60,1))\n",
    "\n",
    "        x_seq_1_avg_pool = self.avg_pool_1(torch.transpose(x_seq_1, 1, 2))\n",
    "        x_seq_1_avg_pool = torch.transpose(x_seq_1_avg_pool,1, 2)\n",
    "        \n",
    "        x_seq_1 = F.elu(self.Linear_2(torch.cat((x_seq_1,x_seq_1_mean,x_seq,x_seq_1_avg_pool),dim=-1)/5))\n",
    "        \n",
    "        att_weight = F.softmax(self.Linear_3_0(x_seq_1) - 10,dim=1)\n",
    "        \n",
    "        \n",
    "        x_seq_out = self.Linear_3(x_seq_1)\n",
    "        x_seq_out = torch.transpose(x_seq_out, 1, 2)\n",
    "        x_seq_out = x_seq_out.reshape((-1,60*len(seq_y_list)))\n",
    "        \n",
    "        x_num_out = F.elu(self.Linear_4_0(torch.sum(att_weight*x_seq_1,dim=1)))\n",
    "        x_num_out = self.Linear_4(x_num_out)\n",
    "        \n",
    "        # print(self.weight)\n",
    "        \n",
    "        return self.weight.weight*(torch.concat((x_seq_out,x_num_out),dim=-1))/3+self.bias.weight/3\n",
    "\n",
    "\n",
    "\n",
    "class FFNN_LSTM_888_AVG_ATT(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(FFNN_LSTM_888_AVG_ATT, self).__init__()\n",
    "        \n",
    "        self.encode_dim = 256\n",
    "        self.hidden_dim = 256\n",
    "        self.iter_dim = 800\n",
    "\n",
    "        \n",
    "        self.LSTM_1 = LSTM(self.encode_dim,self.hidden_dim,6,batch_first=True,dropout=0.01,bidirectional=True)\n",
    "        self.input_size = input_size\n",
    "        self.Linear_1 = nn.Linear(len(seq_fea_list)+len(num_fea_list), self.encode_dim)\n",
    "        self.Linear_2 = nn.Linear(6*self.hidden_dim+self.encode_dim, self.iter_dim)\n",
    "        self.Linear_3 = nn.Linear(self.iter_dim, len(seq_y_list))\n",
    "        self.Linear_3_0 = nn.Linear(self.iter_dim, 1)\n",
    "\n",
    "        self.Linear_4_0 = nn.Linear(self.iter_dim, self.iter_dim*2)\n",
    "\n",
    "        self.Linear_4 = nn.Linear(self.iter_dim*2, len(num_y_list))\n",
    "        self.bias = nn.Linear(len(seq_y_list)*60+len(num_y_list),1)\n",
    "        self.weight = nn.Linear(len(seq_y_list)*60+len(num_y_list),1)\n",
    "        self.avg_pool_1 = AvgPool1d(kernel_size=3,stride=1,padding=1)\n",
    "    def forward(self, x):\n",
    "        x_seq = x[:,0:60*len(seq_fea_list)]\n",
    "        # print(x_seq)\n",
    "        x_num = x[:,60*len(seq_fea_list):x.shape[1]]\n",
    "        x_seq = x_seq.reshape((-1,len(seq_fea_list),60))\n",
    "        # print(x_seq)\n",
    "        x_seq = torch.transpose(x_seq, 1, 2)\n",
    "        x_num_repeat = x_num.reshape((-1,1,len(num_fea_list)))\n",
    "        x_num_repeat = x_num_repeat.repeat((1,60,1))\n",
    "        \n",
    "        # print(x_num_repeat.shape)\n",
    "        # print(x_seq.shape)\n",
    "\n",
    "        \n",
    "        x_seq = F.elu(self.Linear_1(torch.concat((x_seq,x_num_repeat),dim=-1)/5))\n",
    "        \n",
    "        x_seq_1,_ = self.LSTM_1(x_seq/5)\n",
    "        \n",
    "        x_seq_1_mean = torch.mean(x_seq_1,dim=1,keepdim=True)\n",
    "        x_seq_1_mean = x_seq_1_mean.repeat((1,60,1))\n",
    "\n",
    "        x_seq_1_avg_pool = self.avg_pool_1(torch.transpose(x_seq_1, 1, 2))\n",
    "        x_seq_1_avg_pool = torch.transpose(x_seq_1_avg_pool,1, 2)\n",
    "        \n",
    "        x_seq_1 = F.elu(self.Linear_2(torch.cat((x_seq_1,x_seq_1_mean,x_seq,x_seq_1_avg_pool),dim=-1)/5))\n",
    "        \n",
    "        att_weight = F.softmax(self.Linear_3_0(x_seq_1) - 10,dim=1)\n",
    "        \n",
    "        \n",
    "        x_seq_out = self.Linear_3(x_seq_1)\n",
    "        x_seq_out = torch.transpose(x_seq_out, 1, 2)\n",
    "        x_seq_out = x_seq_out.reshape((-1,60*len(seq_y_list)))\n",
    "        \n",
    "        x_num_out = F.elu(self.Linear_4_0(torch.sum(att_weight*x_seq_1,dim=1)))\n",
    "        x_num_out = self.Linear_4(x_num_out)\n",
    "        \n",
    "        # print(self.weight)\n",
    "        \n",
    "        return self.weight.weight*(torch.concat((x_seq_out,x_num_out),dim=-1))/3+self.bias.weight/3\n",
    "\n",
    "\n",
    "class FFNN_LSTM_999_AVG_ATT(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(FFNN_LSTM_999_AVG_ATT, self).__init__()\n",
    "        \n",
    "        self.encode_dim = 256\n",
    "        self.hidden_dim = 300\n",
    "        self.iter_dim = 800\n",
    "\n",
    "        \n",
    "        self.LSTM_1 = LSTM(self.encode_dim,self.hidden_dim,7,batch_first=True,dropout=0.01,bidirectional=True)\n",
    "        self.input_size = input_size\n",
    "        self.Linear_1 = nn.Linear(len(seq_fea_list)+len(num_fea_list), self.encode_dim)\n",
    "        self.Linear_2 = nn.Linear(6*self.hidden_dim+self.encode_dim, self.iter_dim)\n",
    "        self.Linear_3 = nn.Linear(self.iter_dim, len(seq_y_list))\n",
    "        self.Linear_3_0 = nn.Linear(self.iter_dim, 1)\n",
    "\n",
    "        self.Linear_4_0 = nn.Linear(self.iter_dim, self.iter_dim*2)\n",
    "\n",
    "        self.Linear_4 = nn.Linear(self.iter_dim*2, len(num_y_list))\n",
    "        self.bias = nn.Linear(len(seq_y_list)*60+len(num_y_list),1)\n",
    "        self.weight = nn.Linear(len(seq_y_list)*60+len(num_y_list),1)\n",
    "        self.avg_pool_1 = AvgPool1d(kernel_size=3,stride=1,padding=1)\n",
    "    def forward(self, x):\n",
    "        x_seq = x[:,0:60*len(seq_fea_list)]\n",
    "        # print(x_seq)\n",
    "        x_num = x[:,60*len(seq_fea_list):x.shape[1]]\n",
    "        x_seq = x_seq.reshape((-1,len(seq_fea_list),60))\n",
    "        # print(x_seq)\n",
    "        x_seq = torch.transpose(x_seq, 1, 2)\n",
    "        x_num_repeat = x_num.reshape((-1,1,len(num_fea_list)))\n",
    "        x_num_repeat = x_num_repeat.repeat((1,60,1))\n",
    "        \n",
    "        # print(x_num_repeat.shape)\n",
    "        # print(x_seq.shape)\n",
    "\n",
    "        \n",
    "        x_seq = F.elu(self.Linear_1(torch.concat((x_seq,x_num_repeat),dim=-1)/5))\n",
    "        \n",
    "        x_seq_1,_ = self.LSTM_1(x_seq/5)\n",
    "        \n",
    "        x_seq_1_mean = torch.mean(x_seq_1,dim=1,keepdim=True)\n",
    "        x_seq_1_mean = x_seq_1_mean.repeat((1,60,1))\n",
    "\n",
    "        x_seq_1_avg_pool = self.avg_pool_1(torch.transpose(x_seq_1, 1, 2))\n",
    "        x_seq_1_avg_pool = torch.transpose(x_seq_1_avg_pool,1, 2)\n",
    "        \n",
    "        x_seq_1 = F.elu(self.Linear_2(torch.cat((x_seq_1,x_seq_1_mean,x_seq,x_seq_1_avg_pool),dim=-1)/5))\n",
    "        \n",
    "        att_weight = F.softmax(self.Linear_3_0(x_seq_1) - 10,dim=1)\n",
    "        \n",
    "        \n",
    "        x_seq_out = self.Linear_3(x_seq_1)\n",
    "        x_seq_out = torch.transpose(x_seq_out, 1, 2)\n",
    "        x_seq_out = x_seq_out.reshape((-1,60*len(seq_y_list)))\n",
    "        \n",
    "        x_num_out = F.elu(self.Linear_4_0(torch.sum(att_weight*x_seq_1,dim=1)))\n",
    "        x_num_out = self.Linear_4(x_num_out)\n",
    "        \n",
    "        # print(self.weight)\n",
    "        \n",
    "        return self.weight.weight*(torch.concat((x_seq_out,x_num_out),dim=-1))/3+self.bias.weight/3\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class FFNN_LSTM_4_AVG_ATT(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(FFNN_LSTM_4_AVG_ATT, self).__init__()\n",
    "        \n",
    "        self.encode_dim = 380\n",
    "        self.hidden_dim = 380\n",
    "        self.iter_dim = 1024\n",
    "\n",
    "        \n",
    "        self.LSTM_1 = LSTM(self.encode_dim,self.hidden_dim,4,batch_first=True,dropout=0.05,bidirectional=True)\n",
    "        self.input_size = input_size\n",
    "        self.Linear_1 = nn.Linear(len(seq_fea_list)+len(num_fea_list), self.encode_dim)\n",
    "        self.Linear_2 = nn.Linear(6*self.hidden_dim+self.encode_dim, self.iter_dim)\n",
    "        self.Linear_3 = nn.Linear(self.iter_dim, len(seq_y_list))\n",
    "        self.Linear_3_0 = nn.Linear(self.iter_dim, 1)\n",
    "\n",
    "        self.Linear_4_0 = nn.Linear(self.iter_dim, self.iter_dim*2)\n",
    "\n",
    "        self.Linear_4 = nn.Linear(self.iter_dim*2, len(num_y_list))\n",
    "        self.bias = nn.Linear(len(seq_y_list)*60+len(num_y_list),1)\n",
    "        self.weight = nn.Linear(len(seq_y_list)*60+len(num_y_list),1)\n",
    "        self.avg_pool_1 = AvgPool1d(kernel_size=3,stride=1,padding=1)\n",
    "    def forward(self, x):\n",
    "        x_seq = x[:,0:60*len(seq_fea_list)]\n",
    "        # print(x_seq)\n",
    "        x_num = x[:,60*len(seq_fea_list):x.shape[1]]\n",
    "        x_seq = x_seq.reshape((-1,len(seq_fea_list),60))\n",
    "        # print(x_seq)\n",
    "        x_seq = torch.transpose(x_seq, 1, 2)\n",
    "        x_num_repeat = x_num.reshape((-1,1,len(num_fea_list)))\n",
    "        x_num_repeat = x_num_repeat.repeat((1,60,1))\n",
    "        \n",
    "        # print(x_num_repeat.shape)\n",
    "        # print(x_seq.shape)\n",
    "\n",
    "        \n",
    "        x_seq = F.elu(self.Linear_1(torch.concat((x_seq,x_num_repeat),dim=-1)/5))\n",
    "        \n",
    "        x_seq_1,_ = self.LSTM_1(x_seq/5)\n",
    "        \n",
    "        x_seq_1_mean = torch.mean(x_seq_1,dim=1,keepdim=True)\n",
    "        x_seq_1_mean = x_seq_1_mean.repeat((1,60,1))\n",
    "\n",
    "        x_seq_1_avg_pool = self.avg_pool_1(torch.transpose(x_seq_1, 1, 2))\n",
    "        x_seq_1_avg_pool = torch.transpose(x_seq_1_avg_pool,1, 2)\n",
    "        \n",
    "        x_seq_1 = F.elu(self.Linear_2(torch.cat((x_seq_1,x_seq_1_mean,x_seq,x_seq_1_avg_pool),dim=-1)/5))\n",
    "        \n",
    "        att_weight = F.softmax(self.Linear_3_0(x_seq_1) - 10,dim=1)\n",
    "        \n",
    "        \n",
    "        x_seq_out = self.Linear_3(x_seq_1)\n",
    "        x_seq_out = torch.transpose(x_seq_out, 1, 2)\n",
    "        x_seq_out = x_seq_out.reshape((-1,60*len(seq_y_list)))\n",
    "        \n",
    "        x_num_out = F.elu(self.Linear_4_0(torch.sum(att_weight*x_seq_1,dim=1)))\n",
    "        x_num_out = self.Linear_4(x_num_out)\n",
    "        \n",
    "        # print(self.weight)\n",
    "        \n",
    "        return self.weight.weight*(torch.concat((x_seq_out,x_num_out),dim=-1))/3+self.bias.weight/3\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class FFNN_UNET_4(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(FFNN_UNET_4, self).__init__()\n",
    "        \n",
    "        self.encode_dim = 150\n",
    "        self.hidden_dim = 220\n",
    "        self.iter_dim = 450\n",
    "        \n",
    "\n",
    "        self.enc_channels= (self.encode_dim, self.encode_dim*2, self.encode_dim*4, self.encode_dim*8)\n",
    "        self.dec_channels= (self.encode_dim*8, self.encode_dim*4, self.encode_dim*2)\n",
    "\n",
    "        self.encoder = Encoder(self.enc_channels)\n",
    "        self.decoder = Decoder(self.dec_channels)\n",
    "        \n",
    "        # self.LSTM_1 = LSTM(self.encode_dim,self.hidden_dim,5,batch_first=True,dropout=0.05,bidirectional=True)\n",
    "        self.input_size = input_size\n",
    "        self.Linear_1 = nn.Linear(len(seq_fea_list)+len(num_fea_list), self.encode_dim)\n",
    "        self.Linear_2 = nn.Linear(6*self.encode_dim+self.encode_dim, self.iter_dim)\n",
    "        self.Linear_3 = nn.Linear(self.iter_dim, len(seq_y_list))\n",
    "        \n",
    "        self.Linear_4_0 = nn.Linear(self.iter_dim, self.iter_dim*2)\n",
    "\n",
    "        self.Linear_4 = nn.Linear(self.iter_dim*2, len(num_y_list))\n",
    "   \n",
    "        \n",
    "        \n",
    "        # self.Linear_4 = nn.Linear(len(seq_fea_list)*60+len(num_fea_list)+60*self.iter_dim+len(num_fea_list), 2*self.iter_dim)\n",
    "        self.Linear_5 = nn.Linear(2*self.iter_dim,len(num_y_list))\n",
    "        self.bias = nn.Linear(len(seq_y_list)*60+len(num_y_list),1)\n",
    "        self.weight = nn.Linear(len(seq_y_list)*60+len(num_y_list),1)\n",
    "        self.avg_pool_1 = AvgPool1d(kernel_size=3,stride=1,padding=1)\n",
    "    def forward(self, x):\n",
    "        x_seq = x[:,0:60*len(seq_fea_list)]\n",
    "        x_num = x[:,60*len(seq_fea_list):x.shape[1]]\n",
    "        x_seq = x_seq.reshape((-1,len(seq_fea_list),60))\n",
    "        x_seq = torch.transpose(x_seq, 1, 2)\n",
    "        x_num_repeat = x_num.reshape((-1,1,len(num_fea_list)))\n",
    "        x_num_repeat = x_num_repeat.repeat((1,60,1))\n",
    "        \n",
    "        x_seq = F.elu(self.Linear_1(torch.concat((x_seq,x_num_repeat),dim=-1)/5))\n",
    "        \n",
    "        enc_ftrs = self.encoder(torch.transpose(x_seq,1, 2))\n",
    "        x_seq_1 = self.decoder(enc_ftrs[::-1][0], enc_ftrs[::-1][1:])\n",
    "        x_seq_1 = torch.transpose(x_seq_1, 1, 2)\n",
    "        \n",
    "        # print(x_seq_1.shape)\n",
    "        \n",
    "        x_seq_1_mean = torch.mean(x_seq_1,dim=1,keepdim=True)\n",
    "        x_seq_1_mean = x_seq_1_mean.repeat((1,60,1))\n",
    "\n",
    "        x_seq_1_avg_pool = self.avg_pool_1(torch.transpose(x_seq_1, 1, 2))\n",
    "        x_seq_1_avg_pool = torch.transpose(x_seq_1_avg_pool,1, 2)\n",
    "        \n",
    "        x_seq_1 = F.elu(self.Linear_2(torch.cat((x_seq_1,x_seq_1_mean,x_seq,x_seq_1_avg_pool),dim=-1)/5))\n",
    "        \n",
    "        x_seq_out = self.Linear_3(x_seq_1)\n",
    "        x_seq_out = torch.transpose(x_seq_out, 1, 2)\n",
    "        x_seq_out = x_seq_out.reshape((-1,60*len(seq_y_list)))\n",
    "        \n",
    "        x_num_out = F.elu(self.Linear_4_0(torch.mean(x_seq_1,dim=1)))\n",
    "        x_num_out = self.Linear_4(x_num_out)\n",
    "\n",
    "        output = self.weight.weight*(torch.concat((x_seq_out,x_num_out),dim=-1))/3+self.bias.weight/3\n",
    "\n",
    "        output[:,zeroout_index] =  output[:,zeroout_index]*0.0\n",
    "\n",
    "        \n",
    "        \n",
    "        # print(self.weight)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "class FFNN_LSTM_6_AVG(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(FFNN_LSTM_6_AVG, self).__init__()\n",
    "        \n",
    "        self.encode_dim = 300\n",
    "        self.hidden_dim = 280\n",
    "        self.iter_dim = 800\n",
    "\n",
    "        \n",
    "        self.LSTM_1 = LSTM(self.encode_dim,self.hidden_dim,6,batch_first=True,dropout=0.01,bidirectional=True)\n",
    "        self.input_size = input_size\n",
    "        self.Linear_1 = nn.Linear(len(seq_fea_list)+len(num_fea_list), self.encode_dim)\n",
    "        self.Linear_2 = nn.Linear(6*self.hidden_dim+self.encode_dim, self.iter_dim)\n",
    "        self.Linear_3 = nn.Linear(self.iter_dim, len(seq_y_list))\n",
    "        self.Linear_4_0 = nn.Linear(self.iter_dim, self.iter_dim*2)\n",
    "\n",
    "        self.Linear_4 = nn.Linear(self.iter_dim*2, len(num_y_list))\n",
    "        self.bias = nn.Linear(len(seq_y_list)*60+len(num_y_list),1)\n",
    "        self.weight = nn.Linear(len(seq_y_list)*60+len(num_y_list),1)\n",
    "        self.avg_pool_1 = AvgPool1d(kernel_size=3,stride=1,padding=1)\n",
    "    def forward(self, x):\n",
    "        x_seq = x[:,0:60*len(seq_fea_list)]\n",
    "        x_num = x[:,60*len(seq_fea_list):x.shape[1]]\n",
    "        x_seq = x_seq.reshape((-1,len(seq_fea_list),60))\n",
    "        x_seq = torch.transpose(x_seq, 1, 2)\n",
    "        x_num_repeat = x_num.reshape((-1,1,len(num_fea_list)))\n",
    "        x_num_repeat = x_num_repeat.repeat((1,60,1))\n",
    "        \n",
    "        x_seq = F.elu(self.Linear_1(torch.concat((x_seq,x_num_repeat),dim=-1)/5))\n",
    "        \n",
    "        x_seq_1,_ = self.LSTM_1(x_seq/5)\n",
    "        \n",
    "        x_seq_1_mean = torch.mean(x_seq_1,dim=1,keepdim=True)\n",
    "        x_seq_1_mean = x_seq_1_mean.repeat((1,60,1))\n",
    "\n",
    "        x_seq_1_avg_pool = self.avg_pool_1(torch.transpose(x_seq_1, 1, 2))\n",
    "        x_seq_1_avg_pool = torch.transpose(x_seq_1_avg_pool,1, 2)\n",
    "        \n",
    "        x_seq_1 = F.elu(self.Linear_2(torch.cat((x_seq_1,x_seq_1_mean,x_seq,x_seq_1_avg_pool),dim=-1)/5))\n",
    "        \n",
    "        x_seq_out = self.Linear_3(x_seq_1)\n",
    "        x_seq_out = torch.transpose(x_seq_out, 1, 2)\n",
    "        x_seq_out = x_seq_out.reshape((-1,60*len(seq_y_list)))\n",
    "        \n",
    "        x_num_out = F.elu(self.Linear_4_0(torch.mean(x_seq_1,dim=1)))\n",
    "        x_num_out = self.Linear_4(x_num_out)\n",
    "\n",
    "        output = self.weight.weight*(torch.concat((x_seq_out,x_num_out),dim=-1))/3+self.bias.weight/3\n",
    "        \n",
    "        output[:,zeroout_index] =  output[:,zeroout_index]*0.0\n",
    "        # print(self.weight)\n",
    "        \n",
    "        return output\n",
    "\n",
    " \n",
    "class FFNN_LSTM_6_AVG_4(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(FFNN_LSTM_6_AVG_4, self).__init__()\n",
    "        \n",
    "        self.encode_dim = 300\n",
    "        self.hidden_dim = 400\n",
    "        self.iter_dim = 1400\n",
    "\n",
    "        \n",
    "        self.LSTM_1 = LSTM(self.encode_dim,self.hidden_dim,6,batch_first=True,dropout=0.05,bidirectional=True)\n",
    "        self.input_size = input_size\n",
    "        self.Linear_1 = nn.Linear(len(seq_fea_list)+len(num_fea_list), self.encode_dim)\n",
    "        self.Linear_2 = nn.Linear(6*self.hidden_dim+self.encode_dim, self.iter_dim)\n",
    "        \n",
    "        self.Linear_3 = nn.Linear(self.iter_dim, len(seq_y_list))\n",
    "        self.Linear_3_0 = nn.Linear(self.iter_dim, self.iter_dim)\n",
    "        self.Linear_4_0 = nn.Linear(self.iter_dim, self.iter_dim*2)\n",
    "\n",
    "        self.Linear_4 = nn.Linear(self.iter_dim*2, len(num_y_list))\n",
    "        self.bias = nn.Linear(len(seq_y_list)*60+len(num_y_list),1)\n",
    "        self.weight = nn.Linear(len(seq_y_list)*60+len(num_y_list),1)\n",
    "        self.avg_pool_1 = AvgPool1d(kernel_size=3,stride=1,padding=1)\n",
    "    def forward(self, x):\n",
    "        x_seq = x[:,0:60*len(seq_fea_list)]\n",
    "        x_num = x[:,60*len(seq_fea_list):x.shape[1]]\n",
    "        x_seq = x_seq.reshape((-1,len(seq_fea_list),60))\n",
    "        x_seq = torch.transpose(x_seq, 1, 2)\n",
    "        x_num_repeat = x_num.reshape((-1,1,len(num_fea_list)))\n",
    "        x_num_repeat = x_num_repeat.repeat((1,60,1))\n",
    "        \n",
    "        x_seq = F.elu(self.Linear_1(torch.concat((x_seq,x_num_repeat),dim=-1)/5))\n",
    "        \n",
    "        x_seq_1,_ = self.LSTM_1(x_seq/5)\n",
    "        \n",
    "        x_seq_1_mean = torch.mean(x_seq_1,dim=1,keepdim=True)\n",
    "        x_seq_1_mean = x_seq_1_mean.repeat((1,60,1))\n",
    "\n",
    "        x_seq_1_avg_pool = self.avg_pool_1(torch.transpose(x_seq_1, 1, 2))\n",
    "        x_seq_1_avg_pool = torch.transpose(x_seq_1_avg_pool,1, 2)\n",
    "        \n",
    "        x_seq_1 = F.elu(self.Linear_2(torch.cat((x_seq_1,x_seq_1_mean,x_seq,x_seq_1_avg_pool),dim=-1)/5))\n",
    "        \n",
    "        x_seq_out = self.Linear_3(x_seq_1)\n",
    "        x_seq_out = self.Linear_3_0(F.elu(x_seq_out)) + x_seq_out\n",
    "        x_seq_out = torch.transpose(x_seq_out, 1, 2)\n",
    "        x_seq_out = x_seq_out.reshape((-1,60*len(seq_y_list)))\n",
    "        \n",
    "        x_num_out = F.elu(self.Linear_4_0(torch.mean(x_seq_1,dim=1)))\n",
    "        x_num_out = self.Linear_4(x_num_out)\n",
    "\n",
    "        output = self.weight.weight*(torch.concat((x_seq_out,x_num_out),dim=-1))/3+self.bias.weight/3\n",
    "        \n",
    "        output[:,zeroout_index] =  output[:,zeroout_index]*0.0\n",
    "        # print(self.weight)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class FFNN_LSTM_6_AVG_1(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(FFNN_LSTM_6_AVG_1, self).__init__()\n",
    "        \n",
    "        self.encode_dim = 300\n",
    "        self.hidden_dim = 300\n",
    "        self.iter_dim = 1400\n",
    "\n",
    "        \n",
    "        self.LSTM_a = LSTM(self.encode_dim,self.hidden_dim,6,batch_first=True,dropout=0.05,bidirectional=True)\n",
    "\n",
    "        self.Seq_0_Linear_1 = nn.Linear(1, self.encode_dim)\n",
    "        self.Seq_0_Linear_2 = nn.Linear(self.encode_dim,1)\n",
    "\n",
    "        self.Seq_1_Linear_1 = nn.Linear(1, self.encode_dim)\n",
    "        self.Seq_1_Linear_2 = nn.Linear(self.encode_dim,1)\n",
    "\n",
    "        self.Seq_2_Linear_1 = nn.Linear(1, self.encode_dim)\n",
    "        self.Seq_2_Linear_2 = nn.Linear(self.encode_dim,1)\n",
    "\n",
    "        self.Seq_3_Linear_1 = nn.Linear(1, self.encode_dim)\n",
    "        self.Seq_3_Linear_2 = nn.Linear(self.encode_dim,1)\n",
    "\n",
    "        self.Seq_4_Linear_1 = nn.Linear(1, self.encode_dim)\n",
    "        self.Seq_4_Linear_2 = nn.Linear(self.encode_dim,1)\n",
    "\n",
    "        self.Seq_5_Linear_1 = nn.Linear(1, self.encode_dim)\n",
    "        self.Seq_5_Linear_2 = nn.Linear(self.encode_dim,1)\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.Linear_1 = nn.Linear(len(seq_fea_list)+len(num_fea_list), self.encode_dim)\n",
    "        self.Linear_2 = nn.Linear(6*self.hidden_dim+self.encode_dim, self.iter_dim)\n",
    "        self.Linear_3 = nn.Linear(self.iter_dim, len(seq_y_list))\n",
    "        self.Linear_4_0 = nn.Linear(self.iter_dim, self.iter_dim*2)\n",
    "\n",
    "        self.Linear_4 = nn.Linear(self.iter_dim*2, len(num_y_list))\n",
    "        self.bias = nn.Linear(len(seq_y_list)*60+len(num_y_list),1)\n",
    "        self.weight = nn.Linear(len(seq_y_list)*60+len(num_y_list),1)\n",
    "        self.avg_pool_1 = AvgPool1d(kernel_size=3,stride=1,padding=1)\n",
    "    def forward(self, x):\n",
    "        x_seq = x[:,0:60*len(seq_fea_list)]\n",
    "        x_num = x[:,60*len(seq_fea_list):x.shape[1]]\n",
    "        x_seq = x_seq.reshape((-1,len(seq_fea_list),60))\n",
    "\n",
    "        x_seq_1d_out = x_seq*0\n",
    "\n",
    "        x_seq_1d_out[:,:,0] = self.Seq_0_Linear_2(F.elu(self.Seq_0_Linear_1(x_seq[:,:,0][:,:,None])))[:,:,0]\n",
    "        x_seq_1d_out[:,:,1] = self.Seq_1_Linear_2(F.elu(self.Seq_1_Linear_1(x_seq[:,:,1][:,:,None])))[:,:,0]\n",
    "        x_seq_1d_out[:,:,2] = self.Seq_2_Linear_2(F.elu(self.Seq_2_Linear_1(x_seq[:,:,2][:,:,None])))[:,:,0]\n",
    "        x_seq_1d_out[:,:,3] = self.Seq_3_Linear_2(F.elu(self.Seq_3_Linear_1(x_seq[:,:,3][:,:,None])))[:,:,0]\n",
    "        x_seq_1d_out[:,:,4] = self.Seq_4_Linear_2(F.elu(self.Seq_4_Linear_1(x_seq[:,:,4][:,:,None])))[:,:,0]\n",
    "        x_seq_1d_out[:,:,5] = self.Seq_5_Linear_2(F.elu(self.Seq_5_Linear_1(x_seq[:,:,5][:,:,None])))[:,:,0]\n",
    "\n",
    "        x_seq_1d_out = x_seq_1d_out.reshape((-1,len(seq_fea_list)*60))\n",
    "        \n",
    "        x_seq = torch.transpose(x_seq, 1, 2)\n",
    "        x_num_repeat = x_num.reshape((-1,1,len(num_fea_list)))\n",
    "        x_num_repeat = x_num_repeat.repeat((1,60,1))\n",
    "        \n",
    "        x_seq = F.elu(self.Linear_1(torch.concat((x_seq,x_num_repeat),dim=-1)/5))\n",
    "        \n",
    "        x_seq_1,_ = self.LSTM_a(x_seq/5)\n",
    "        \n",
    "        x_seq_1_mean = torch.mean(x_seq_1,dim=1,keepdim=True)\n",
    "        x_seq_1_mean = x_seq_1_mean.repeat((1,60,1))\n",
    "\n",
    "        x_seq_1_avg_pool = self.avg_pool_1(torch.transpose(x_seq_1, 1, 2))\n",
    "        x_seq_1_avg_pool = torch.transpose(x_seq_1_avg_pool,1, 2)\n",
    "        \n",
    "        x_seq_1 = F.elu(self.Linear_2(torch.cat((x_seq_1,x_seq_1_mean,x_seq,x_seq_1_avg_pool),dim=-1)/5))\n",
    "        \n",
    "        x_seq_out = self.Linear_3(x_seq_1)\n",
    "        x_seq_out = torch.transpose(x_seq_out, 1, 2)\n",
    "        x_seq_out = 0.0000001*x_seq_out.reshape((-1,60*len(seq_y_list))) + x_seq_1d_out\n",
    "        \n",
    "        x_num_out = F.elu(self.Linear_4_0(torch.mean(x_seq_1,dim=1)))\n",
    "        x_num_out = self.Linear_4(x_num_out)\n",
    "\n",
    "        output = self.weight.weight*(torch.concat((x_seq_out,x_num_out),dim=-1))/3+self.bias.weight/3\n",
    "        \n",
    "        output[:,zeroout_index] =  output[:,zeroout_index]*0.0\n",
    "        # print(self.weight)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "class FFNN_LSTM_6_AVG_2(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(FFNN_LSTM_6_AVG_2, self).__init__()\n",
    "        \n",
    "        self.encode_dim = 300\n",
    "        self.hidden_dim = 300\n",
    "        self.iter_dim = 1400\n",
    "\n",
    "        self.small_hidden_dim = 30\n",
    "        \n",
    "        self.LSTM_a = LSTM(self.encode_dim,self.hidden_dim,6,batch_first=True,dropout=0.05,bidirectional=True)\n",
    "\n",
    "        self.LSTM_0 = LSTM(1,self.small_hidden_dim,3,batch_first=True,dropout=0.05,bidirectional=True)\n",
    "        self.LSTM_1 = LSTM(1,self.small_hidden_dim,3,batch_first=True,dropout=0.05,bidirectional=True)\n",
    "        self.LSTM_2 = LSTM(1,self.small_hidden_dim,3,batch_first=True,dropout=0.05,bidirectional=True)\n",
    "        self.LSTM_3 = LSTM(1,self.small_hidden_dim,3,batch_first=True,dropout=0.05,bidirectional=True)\n",
    "        self.LSTM_4 = LSTM(1,self.small_hidden_dim,3,batch_first=True,dropout=0.05,bidirectional=True)\n",
    "        self.LSTM_5 = LSTM(1,self.small_hidden_dim,3,batch_first=True,dropout=0.05,bidirectional=True)\n",
    "\n",
    "        self.LSTM_0_Linear = nn.Linear(self.small_hidden_dim*2, 1)\n",
    "        self.LSTM_1_Linear = nn.Linear(self.small_hidden_dim*2, 1)\n",
    "        self.LSTM_2_Linear = nn.Linear(self.small_hidden_dim*2, 1)\n",
    "        self.LSTM_3_Linear = nn.Linear(self.small_hidden_dim*2, 1)\n",
    "        self.LSTM_4_Linear = nn.Linear(self.small_hidden_dim*2, 1)\n",
    "        self.LSTM_5_Linear = nn.Linear(self.small_hidden_dim*2, 1)\n",
    "        \n",
    "        self.Seq_0_Linear_1 = nn.Linear(1, self.encode_dim)\n",
    "        self.Seq_0_Linear_2 = nn.Linear(self.encode_dim,1)\n",
    "\n",
    "        self.Seq_1_Linear_1 = nn.Linear(1, self.encode_dim)\n",
    "        self.Seq_1_Linear_2 = nn.Linear(self.encode_dim,1)\n",
    "\n",
    "        self.Seq_2_Linear_1 = nn.Linear(1, self.encode_dim)\n",
    "        self.Seq_2_Linear_2 = nn.Linear(self.encode_dim,1)\n",
    "\n",
    "        self.Seq_3_Linear_1 = nn.Linear(1, self.encode_dim)\n",
    "        self.Seq_3_Linear_2 = nn.Linear(self.encode_dim,1)\n",
    "\n",
    "        self.Seq_4_Linear_1 = nn.Linear(1, self.encode_dim)\n",
    "        self.Seq_4_Linear_2 = nn.Linear(self.encode_dim,1)\n",
    "\n",
    "        self.Seq_5_Linear_1 = nn.Linear(1, self.encode_dim)\n",
    "        self.Seq_5_Linear_2 = nn.Linear(self.encode_dim,1)\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.Linear_1 = nn.Linear(len(seq_fea_list)+len(num_fea_list), self.encode_dim)\n",
    "        self.Linear_2 = nn.Linear(6*self.hidden_dim+self.encode_dim, self.iter_dim)\n",
    "        self.Linear_3 = nn.Linear(self.iter_dim, len(seq_y_list))\n",
    "        self.Linear_4_0 = nn.Linear(self.iter_dim, self.iter_dim*2)\n",
    "\n",
    "        self.Linear_4 = nn.Linear(self.iter_dim*2, len(num_y_list))\n",
    "        self.bias = nn.Linear(len(seq_y_list)*60+len(num_y_list),1)\n",
    "        self.weight = nn.Linear(len(seq_y_list)*60+len(num_y_list),1)\n",
    "        self.avg_pool_1 = AvgPool1d(kernel_size=3,stride=1,padding=1)\n",
    "    def forward(self, x):\n",
    "        x_seq = x[:,0:60*len(seq_fea_list)]\n",
    "        x_num = x[:,60*len(seq_fea_list):x.shape[1]]\n",
    "        x_seq = x_seq.reshape((-1,len(seq_fea_list),60))\n",
    "\n",
    "        x_seq_1d_out = x_seq[:,0:6,:]*0\n",
    "        x_seq_1d_out = torch.transpose(x_seq_1d_out, 1, 2)\n",
    "\n",
    "        tmp_0 = self.Seq_0_Linear_2(F.elu(self.Seq_0_Linear_1(x_seq_1d_out[:,:,0][:,:,None])))[:,:,0]\n",
    "        tmp_1 = self.Seq_1_Linear_2(F.elu(self.Seq_1_Linear_1(x_seq_1d_out[:,:,1][:,:,None])))[:,:,0]\n",
    "        tmp_2 = self.Seq_2_Linear_2(F.elu(self.Seq_2_Linear_1(x_seq_1d_out[:,:,2][:,:,None])))[:,:,0]\n",
    "        tmp_3 = self.Seq_3_Linear_2(F.elu(self.Seq_3_Linear_1(x_seq_1d_out[:,:,3][:,:,None])))[:,:,0]\n",
    "        tmp_4 = self.Seq_4_Linear_2(F.elu(self.Seq_4_Linear_1(x_seq_1d_out[:,:,4][:,:,None])))[:,:,0]\n",
    "        tmp_5 = self.Seq_5_Linear_2(F.elu(self.Seq_5_Linear_1(x_seq_1d_out[:,:,5][:,:,None])))[:,:,0]\n",
    "\n",
    "        x_seq_1d_out = torch.cat((tmp_0[:,:,None],tmp_1[:,:,None],tmp_2[:,:,None],tmp_3[:,:,None],tmp_4[:,:,None],tmp_5[:,:,None]),dim=-1)\n",
    "\n",
    "        \n",
    "        x_seq_1d_out = torch.transpose(x_seq_1d_out, 1, 2)\n",
    "        x_seq_1d_out = x_seq_1d_out.reshape((-1,len(seq_y_list)*60))\n",
    "        \n",
    "        x_seq_2d_out = x_seq[:,0:6,:]*0\n",
    "        x_seq_2d_out = torch.transpose(x_seq_2d_out, 1, 2)\n",
    "        \n",
    "        tmp_0 = self.LSTM_0_Linear(self.LSTM_0(x_seq_2d_out[:,:,0][:,:,None])[0])[:,:,0]\n",
    "        tmp_1 = self.LSTM_1_Linear(self.LSTM_1(x_seq_2d_out[:,:,1][:,:,None])[0])[:,:,0]\n",
    "        tmp_2 = self.LSTM_2_Linear(self.LSTM_2(x_seq_2d_out[:,:,2][:,:,None])[0])[:,:,0]\n",
    "        tmp_3 = self.LSTM_3_Linear(self.LSTM_3(x_seq_2d_out[:,:,3][:,:,None])[0])[:,:,0]\n",
    "        tmp_4 = self.LSTM_4_Linear(self.LSTM_4(x_seq_2d_out[:,:,4][:,:,None])[0])[:,:,0]\n",
    "        tmp_5 = self.LSTM_5_Linear(self.LSTM_5(x_seq_2d_out[:,:,5][:,:,None])[0])[:,:,0]\n",
    "\n",
    "        x_seq_2d_out = torch.cat((tmp_0[:,:,None],tmp_1[:,:,None],tmp_2[:,:,None],tmp_3[:,:,None],tmp_4[:,:,None],tmp_5[:,:,None]),dim=-1)\n",
    "\n",
    "        x_seq_2d_out = torch.transpose(x_seq_2d_out, 1, 2)\n",
    "        x_seq_2d_out = x_seq_2d_out.reshape((-1,len(seq_y_list)*60))\n",
    "\n",
    "        #print(x_seq_1d_out.shape)\n",
    "        #print(x_seq_2d_out.shape)\n",
    "\n",
    "        \n",
    "        x_seq = torch.transpose(x_seq, 1, 2)\n",
    "\n",
    "        \n",
    "        x_num_repeat = x_num.reshape((-1,1,len(num_fea_list)))\n",
    "        x_num_repeat = x_num_repeat.repeat((1,60,1))\n",
    "        \n",
    "        x_seq = F.elu(self.Linear_1(torch.concat((x_seq,x_num_repeat),dim=-1)/5))\n",
    "        \n",
    "        x_seq_1,_ = self.LSTM_a(x_seq/5)\n",
    "        \n",
    "        x_seq_1_mean = torch.mean(x_seq_1,dim=1,keepdim=True)\n",
    "        x_seq_1_mean = x_seq_1_mean.repeat((1,60,1))\n",
    "\n",
    "\n",
    "        x_seq_1_avg_pool = self.avg_pool_1(torch.transpose(x_seq_1, 1, 2))\n",
    "        x_seq_1_avg_pool = torch.transpose(x_seq_1_avg_pool,1, 2)\n",
    "        \n",
    "        x_seq_1 = F.elu(self.Linear_2(torch.cat((x_seq_1,x_seq_1_mean,x_seq,x_seq_1_avg_pool),dim=-1)/5))\n",
    "        \n",
    "        x_seq_out = self.Linear_3(x_seq_1)\n",
    "        x_seq_out = torch.transpose(x_seq_out, 1, 2)\n",
    "        x_seq_out = (x_seq_out.reshape((-1,60*len(seq_y_list)))) + x_seq_1d_out +  (x_seq_2d_out)\n",
    "        \n",
    "        x_num_out = F.elu(self.Linear_4_0(torch.mean(x_seq_1,dim=1)))\n",
    "        x_num_out = self.Linear_4(x_num_out)\n",
    "\n",
    "        output = self.weight.weight*(torch.concat((x_seq_out,x_num_out),dim=-1))/3+self.bias.weight/3\n",
    "        \n",
    "        output[:,zeroout_index] =  output[:,zeroout_index]*0.0\n",
    "        # print(self.weight)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class FFNN_GRU_6_AVG(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(FFNN_GRU_6_AVG, self).__init__()\n",
    "        \n",
    "        self.encode_dim = 290\n",
    "        self.hidden_dim = 280\n",
    "        self.iter_dim = 1024\n",
    "\n",
    "        \n",
    "        self.LSTM_1 = GRU(self.encode_dim,self.hidden_dim,3,batch_first=True,dropout=0.05,bidirectional=True)\n",
    "        self.input_size = input_size\n",
    "        self.Linear_1 = nn.Linear(len(seq_fea_list)+len(num_fea_list), self.encode_dim)\n",
    "        self.Linear_2 = nn.Linear(6*self.hidden_dim+self.encode_dim, self.iter_dim)\n",
    "        self.Linear_2_0 = nn.Linear(self.iter_dim, self.iter_dim)\n",
    "        self.Linear_2_1 = nn.Linear(self.iter_dim, self.iter_dim)\n",
    "\n",
    "\n",
    "        self.Linear_3 = nn.Linear(self.iter_dim, len(seq_y_list))\n",
    "        self.Linear_4_0 = nn.Linear(self.iter_dim, self.iter_dim*2)\n",
    "\n",
    "        self.Linear_4 = nn.Linear(self.iter_dim*2, len(num_y_list))\n",
    "        self.bias = nn.Linear(len(seq_y_list)*60+len(num_y_list),1)\n",
    "        self.weight = nn.Linear(len(seq_y_list)*60+len(num_y_list),1)\n",
    "        self.avg_pool_1 = AvgPool1d(kernel_size=3,stride=1,padding=1)\n",
    "    def forward(self, x):\n",
    "        x_seq = x[:,0:60*len(seq_fea_list)]\n",
    "        x_num = x[:,60*len(seq_fea_list):x.shape[1]]\n",
    "        x_seq = x_seq.reshape((-1,len(seq_fea_list),60))\n",
    "        x_seq = torch.transpose(x_seq, 1, 2)\n",
    "        x_num_repeat = x_num.reshape((-1,1,len(num_fea_list)))\n",
    "        x_num_repeat = x_num_repeat.repeat((1,60,1))\n",
    "        \n",
    "        x_seq = F.elu(self.Linear_1(torch.concat((x_seq,x_num_repeat),dim=-1)/5))\n",
    "        \n",
    "        x_seq_1,_ = self.LSTM_1(x_seq/5)\n",
    "        \n",
    "        x_seq_1_mean = torch.mean(x_seq_1,dim=1,keepdim=True)\n",
    "        x_seq_1_mean = x_seq_1_mean.repeat((1,60,1))\n",
    "\n",
    "        x_seq_1_avg_pool = self.avg_pool_1(torch.transpose(x_seq_1, 1, 2))\n",
    "        x_seq_1_avg_pool = torch.transpose(x_seq_1_avg_pool,1, 2)\n",
    "        \n",
    "        x_seq_1 = F.elu(self.Linear_2(torch.cat((x_seq_1,x_seq_1_mean,x_seq,x_seq_1_avg_pool),dim=-1)/5))\n",
    "        x_seq_1 = x_seq_1 + 0.01*self.Linear_2_1(F.elu(self.Linear_2_0(x_seq_1)))\n",
    "        \n",
    "        x_seq_out = self.Linear_3(x_seq_1)\n",
    "        x_seq_out = torch.transpose(x_seq_out, 1, 2)\n",
    "        x_seq_out = x_seq_out.reshape((-1,60*len(seq_y_list)))\n",
    "        \n",
    "        x_num_out = F.elu(self.Linear_4_0(torch.mean(x_seq_1,dim=1)))\n",
    "        x_num_out = self.Linear_4(x_num_out)\n",
    "        \n",
    "        # print(self.weight)\n",
    "        \n",
    "        return self.weight.weight*(torch.concat((x_seq_out,x_num_out),dim=-1))/3+self.bias.weight/3\n",
    "         \n",
    "\n",
    "\n",
    "class FFNN_LSTM_7(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(FFNN_LSTM_7, self).__init__()\n",
    "        \n",
    "        self.encode_dim = 160\n",
    "        self.hidden_dim = 250\n",
    "        self.iter_dim = 400\n",
    "\n",
    "        \n",
    "        self.LSTM_1 = LSTM(self.encode_dim,self.hidden_dim,7,batch_first=True,dropout=0.05,bidirectional=True)\n",
    "        self.input_size = input_size\n",
    "        self.Linear_1 = nn.Linear(len(seq_fea_list)+len(num_fea_list), self.encode_dim)\n",
    "        self.Linear_2 = nn.Linear(6*self.hidden_dim+self.encode_dim, self.iter_dim)\n",
    "        self.Linear_3 = nn.Linear(self.iter_dim, len(seq_y_list))\n",
    "        self.Linear_4 = nn.Linear(len(seq_fea_list)*60+len(num_fea_list)+60*self.iter_dim+len(num_fea_list), 2*self.iter_dim)\n",
    "        self.Linear_5 = nn.Linear(2*self.iter_dim,len(num_y_list))\n",
    "        self.bias = nn.Linear(len(seq_y_list)*60+len(num_y_list),1)\n",
    "        self.weight = nn.Linear(len(seq_y_list)*60+len(num_y_list),1)\n",
    "        self.avg_pool_1 = AvgPool1d(kernel_size=3,stride=1,padding=1)\n",
    "    def forward(self, x):\n",
    "        x_seq = x[:,0:60*len(seq_fea_list)]\n",
    "        \n",
    "        x_num = x[:,60*len(seq_fea_list):x.shape[1]]\n",
    "        x_seq = x_seq.reshape((-1,len(seq_fea_list),60))\n",
    "        print(x_seq.shape)\n",
    "        x_seq = torch.transpose(x_seq, 1, 2)\n",
    "        x_num_repeat = x_num.reshape((-1,1,len(num_fea_list)))\n",
    "        x_num_repeat = x_num_repeat.repeat((1,60,1))\n",
    "        \n",
    "        x_seq = F.elu(self.Linear_1(torch.concat((x_seq,x_num_repeat),dim=-1)/5))\n",
    "        \n",
    "        x_seq_1,_ = self.LSTM_1(x_seq/5)\n",
    "        \n",
    "        x_seq_1_mean = torch.mean(x_seq_1,dim=1,keepdim=True)\n",
    "        x_seq_1_mean = x_seq_1_mean.repeat((1,60,1))\n",
    "\n",
    "        x_seq_1_avg_pool = self.avg_pool_1(torch.transpose(x_seq_1, 1, 2))\n",
    "        x_seq_1_avg_pool = torch.transpose(x_seq_1_avg_pool,1, 2)\n",
    "        \n",
    "        x_seq_1 = F.elu(self.Linear_2(torch.cat((x_seq_1,x_seq_1_mean,x_seq,x_seq_1_avg_pool),dim=-1)/5))\n",
    "        \n",
    "        x_seq_out = self.Linear_3(x_seq_1)\n",
    "        x_seq_out = torch.transpose(x_seq_out, 1, 2)\n",
    "        x_seq_out = x_seq_out.reshape((-1,60*len(seq_y_list)))\n",
    "        \n",
    "        x_seq_out_1 = x_seq_1.reshape((-1,60*self.iter_dim))\n",
    "        x_num_out = F.elu(self.Linear_4(torch.concat((x_seq_out_1,x_num,x),dim=-1)/5))\n",
    "        x_num_out = self.Linear_5(x_num_out/5)\n",
    "        \n",
    "        # print(self.weight)\n",
    "        \n",
    "        return self.weight.weight*(torch.concat((x_seq_out,x_num_out),dim=-1))/3+self.bias.weight/3\n",
    "       \n",
    "\n",
    "\n",
    "class Conv1dBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(out_channels), \n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, channels=(1, 16, 32, 64)):\n",
    "        super().__init__()\n",
    "        self.enc_blocks = nn.ModuleList([Conv1dBlock(channels[i], channels[i+1]) \n",
    "                                         for i in range(len(channels)-1)])\n",
    "        self.pool = nn.AvgPool1d(2,2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = []\n",
    "        for block in self.enc_blocks:\n",
    "            x = block(x)\n",
    "            features.append(x)\n",
    "            x = self.pool(x)\n",
    "        return features\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, channels=(64, 32, 16)):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.upconvs = nn.ModuleList([nn.ConvTranspose1d(channels[i], channels[i+1], 2, 2) \n",
    "                                      for i in range(len(channels)-1)])\n",
    "        self.dec_blocks = nn.ModuleList([Conv1dBlock(channels[i], channels[i+1]) \n",
    "                                         for i in range(len(channels)-1)])\n",
    "        \n",
    "    def forward(self, x, encoder_features):\n",
    "        for i in range(len(self.channels)-1):\n",
    "            x = self.upconvs[i](x)\n",
    "            enc_ftrs = self.crop(encoder_features[i], x)\n",
    "            x = torch.cat([x, enc_ftrs], dim=1)\n",
    "            x = self.dec_blocks[i](x)\n",
    "        return x\n",
    "    \n",
    "    def crop(self, enc_ftrs, x):\n",
    "        _, _, T = x.shape\n",
    "        enc_ftrs = enc_ftrs[:, :, (enc_ftrs.shape[2] - T) // 2 : (enc_ftrs.shape[2] - T) // 2 + T]\n",
    "        return enc_ftrs\n",
    "\n",
    "class UNet1D(nn.Module):\n",
    "    def __init__(self, enc_channels=(1, 16, 32, 64), dec_channels=(64, 32, 16), nb_classes=1):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(enc_channels)\n",
    "        self.decoder = Decoder(dec_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc_ftrs = self.encoder(x)\n",
    "        out = self.decoder(enc_ftrs[::-1][0], enc_ftrs[::-1][1:])\n",
    "        return out\n",
    "    \n",
    "\n",
    "\n",
    "class FFNN_UNET(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(FFNN_UNET, self).__init__()\n",
    "        \n",
    "        self.encode_dim = 300\n",
    "        self.hidden_dim = 180\n",
    "        self.iter_dim = 300\n",
    "        \n",
    "\n",
    "        self.enc_channels= (self.encode_dim, self.encode_dim*2, self.encode_dim*4, self.encode_dim*8)\n",
    "        self.dec_channels= (self.encode_dim*8, self.encode_dim*4, self.encode_dim*2)\n",
    "\n",
    "        self.encoder = Encoder(self.enc_channels)\n",
    "        self.decoder = Decoder(self.dec_channels)\n",
    "        \n",
    "        # self.LSTM_1 = LSTM(self.encode_dim,self.hidden_dim,5,batch_first=True,dropout=0.05,bidirectional=True)\n",
    "        self.input_size = input_size\n",
    "        self.Linear_1 = nn.Linear(len(seq_fea_list)+len(num_fea_list), self.encode_dim)\n",
    "        self.Linear_2 = nn.Linear(6*self.encode_dim+self.encode_dim, self.iter_dim)\n",
    "        self.Linear_3 = nn.Linear(self.iter_dim, len(seq_y_list))\n",
    "        self.Linear_4 = nn.Linear(len(seq_fea_list)*60+len(num_fea_list)+60*self.iter_dim+len(num_fea_list), 2*self.iter_dim)\n",
    "        self.Linear_5 = nn.Linear(2*self.iter_dim,len(num_y_list))\n",
    "        self.bias = nn.Linear(len(seq_y_list)*60+len(num_y_list),1)\n",
    "        self.weight = nn.Linear(len(seq_y_list)*60+len(num_y_list),1)\n",
    "        self.avg_pool_1 = AvgPool1d(kernel_size=3,stride=1,padding=1)\n",
    "    def forward(self, x):\n",
    "        x_seq = x[:,0:60*len(seq_fea_list)]\n",
    "        x_num = x[:,60*len(seq_fea_list):x.shape[1]]\n",
    "        x_seq = x_seq.reshape((-1,len(seq_fea_list),60))\n",
    "        x_seq = torch.transpose(x_seq, 1, 2)\n",
    "        x_num_repeat = x_num.reshape((-1,1,len(num_fea_list)))\n",
    "        x_num_repeat = x_num_repeat.repeat((1,60,1))\n",
    "        \n",
    "        x_seq = F.elu(self.Linear_1(torch.concat((x_seq,x_num_repeat),dim=-1)/5))\n",
    "        \n",
    "        enc_ftrs = self.encoder(torch.transpose(x_seq,1, 2))\n",
    "        x_seq_1 = self.decoder(enc_ftrs[::-1][0], enc_ftrs[::-1][1:])\n",
    "        x_seq_1 = torch.transpose(x_seq_1, 1, 2)\n",
    "        \n",
    "        # print(x_seq_1.shape)\n",
    "        \n",
    "        x_seq_1_mean = torch.mean(x_seq_1,dim=1,keepdim=True)\n",
    "        x_seq_1_mean = x_seq_1_mean.repeat((1,60,1))\n",
    "\n",
    "        x_seq_1_avg_pool = self.avg_pool_1(torch.transpose(x_seq_1, 1, 2))\n",
    "        x_seq_1_avg_pool = torch.transpose(x_seq_1_avg_pool,1, 2)\n",
    "        \n",
    "        x_seq_1 = F.elu(self.Linear_2(torch.cat((x_seq_1,x_seq_1_mean,x_seq,x_seq_1_avg_pool),dim=-1)/5))\n",
    "        \n",
    "        x_seq_out = self.Linear_3(x_seq_1)\n",
    "        x_seq_out = torch.transpose(x_seq_out, 1, 2)\n",
    "        x_seq_out = x_seq_out.reshape((-1,60*len(seq_y_list)))\n",
    "        \n",
    "        x_seq_out_1 = x_seq_1.reshape((-1,60*self.iter_dim))\n",
    "        x_num_out = F.elu(self.Linear_4(torch.concat((x_seq_out_1,x_num,x),dim=-1)/5))\n",
    "        x_num_out = self.Linear_5(x_num_out/5)\n",
    "        \n",
    "        # print(self.weight)\n",
    "        \n",
    "        return self.weight.weight*(torch.concat((x_seq_out,x_num_out),dim=-1))/3+self.bias.weight/3\n",
    "       \n",
    "\n",
    "\n",
    "\n",
    "class FFNN_Transformer_2(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(FFNN_Transformer_2, self).__init__()\n",
    "        \n",
    "        self.encode_dim = 200\n",
    "        self.hidden_dim = self.encode_dim\n",
    "        self.iter_dim = self.encode_dim\n",
    "        self.encoder_layer_1 = TransformerEncoderLayer(d_model=self.encode_dim, nhead=5,dim_feedforward=512,activation='gelu',batch_first=True)\n",
    "        self.transformer_encoder_1 = TransformerEncoder(self.encoder_layer_1, num_layers=6)  \n",
    "        \n",
    "        self.encoder_layer_2 = TransformerEncoderLayer(d_model=self.encode_dim, nhead=5,dim_feedforward=512,activation='gelu',batch_first=True)\n",
    "        self.transformer_encoder_2 = TransformerEncoder(self.encoder_layer_2, num_layers=3) \n",
    "        self.encoder_layer_unfold_2 = nn.Linear(self.encode_dim,60)\n",
    "        \n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.pos_emb_size = 8\n",
    "        self.pos_emb = nn.Linear(self.pos_emb_size,60)\n",
    "        self.Linear_1 = nn.Linear(self.pos_emb_size+len(seq_fea_list)+len(num_fea_list), self.encode_dim)\n",
    "        self.Linear_1_otherside = nn.Linear(60, self.encode_dim)\n",
    "        \n",
    "        self.Linear_2 = nn.Linear(2*self.hidden_dim+self.encode_dim, self.iter_dim)\n",
    "        self.Linear_3 = nn.Linear(self.iter_dim, len(seq_y_list))\n",
    "        self.Linear_4 = nn.Linear(len(seq_fea_list)*60+len(num_fea_list)+60*self.iter_dim+len(num_fea_list), 2*self.iter_dim)\n",
    "        self.Linear_5 = nn.Linear(2*self.iter_dim,len(num_y_list))\n",
    "        self.bias = nn.Linear(len(seq_y_list)*60+len(num_y_list),1)\n",
    "        self.weight = nn.Linear(len(seq_y_list)*60+len(num_y_list),1)\n",
    "        self.pos_emb_size = 8\n",
    "        self.pos_emb = nn.Linear(self.pos_emb_size,60)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_seq = x[:,0:60*len(seq_fea_list)]\n",
    "        x_num = x[:,60*len(seq_fea_list):x.shape[1]]     \n",
    "        x_seq = x_seq.reshape((-1,len(seq_fea_list),60))\n",
    "        \n",
    "        x_seq_otherside = self.encoder_layer_unfold_2(self.transformer_encoder_2(self.Linear_1_otherside(x_seq)))\n",
    "        \n",
    "        x_seq = torch.transpose(x_seq, 1, 2)\n",
    "        x_num_repeat = x_num.reshape((-1,1,len(num_fea_list)))\n",
    "        x_num_repeat = x_num_repeat.repeat((1,60,1))\n",
    "        \n",
    "        x_pos_emb = self.pos_emb.weight.reshape(1,60,self.pos_emb_size)\n",
    "        x_pos_emb = x_pos_emb.repeat((x.shape[0],1,1))\n",
    "        \n",
    "        x_seq = F.elu(self.Linear_1(torch.concat((x_seq,x_num_repeat,x_pos_emb),dim=-1)/4))\n",
    "        \n",
    "        x_seq_1 = self.transformer_encoder_1(x_seq)\n",
    "        \n",
    "        \n",
    "        \n",
    "        x_seq_1_mean = torch.mean(x_seq_1,dim=1,keepdim=True)\n",
    "        x_seq_1_mean = x_seq_1_mean.repeat((1,60,1))\n",
    "        \n",
    "        x_seq_1 = F.elu(self.Linear_2(torch.cat((x_seq_1,x_seq_1_mean,x_seq),dim=-1)/4))\n",
    "        \n",
    "        x_seq_out = self.Linear_3(x_seq_1) + torch.transpose(x_seq_otherside[:,0:6,:],1,2)\n",
    "        x_seq_out = torch.transpose(x_seq_out, 1, 2)\n",
    "        x_seq_out = x_seq_out.reshape((-1,60*len(seq_y_list)))\n",
    "        \n",
    "        x_seq_out_1 = x_seq_1.reshape((-1,60*self.iter_dim))\n",
    "        x_num_out = F.elu(self.Linear_4(torch.concat((x_seq_out_1,x_num,x),dim=-1)/5))\n",
    "        x_num_out = self.Linear_5(x_num_out/6)\n",
    "        \n",
    "        # print(self.weight)\n",
    "        \n",
    "        return self.weight.weight*(torch.concat((x_seq_out,x_num_out),dim=-1))/3+self.bias.weight/3\n",
    "       \n",
    "\n",
    "class FFNN_UNET_3(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(FFNN_UNET_3, self).__init__()\n",
    "        \n",
    "        self.encode_dim = 259\n",
    "        self.hidden_dim = 200\n",
    "        self.iter_dim = 320\n",
    "        \n",
    "\n",
    "        self.enc_channels= (self.encode_dim, self.encode_dim*2, self.encode_dim*4, self.encode_dim*8)\n",
    "        self.dec_channels= (self.encode_dim*8, self.encode_dim*4, self.encode_dim*2)\n",
    "\n",
    "        self.encoder = Encoder(self.enc_channels)\n",
    "        self.decoder = Decoder(self.dec_channels)\n",
    "        \n",
    "        # self.LSTM_1 = LSTM(self.encode_dim,self.hidden_dim,5,batch_first=True,dropout=0.05,bidirectional=True)\n",
    "        self.input_size = input_size\n",
    "        self.Linear_1 = nn.Linear(len(seq_fea_list)+len(num_fea_list), self.encode_dim)\n",
    "        self.Linear_2 = nn.Linear(6*self.encode_dim+self.encode_dim, self.iter_dim)\n",
    "        self.Linear_3 = nn.Linear(self.iter_dim, len(seq_y_list))\n",
    "        \n",
    "        self.Linear_4_0 = nn.Linear(self.iter_dim, self.iter_dim*2)\n",
    "\n",
    "        self.Linear_4 = nn.Linear(self.iter_dim*2, len(num_y_list))\n",
    "   \n",
    "        \n",
    "        \n",
    "        # self.Linear_4 = nn.Linear(len(seq_fea_list)*60+len(num_fea_list)+60*self.iter_dim+len(num_fea_list), 2*self.iter_dim)\n",
    "        self.Linear_5 = nn.Linear(2*self.iter_dim,len(num_y_list))\n",
    "        self.bias = nn.Linear(len(seq_y_list)*60+len(num_y_list),1)\n",
    "        self.weight = nn.Linear(len(seq_y_list)*60+len(num_y_list),1)\n",
    "        self.avg_pool_1 = AvgPool1d(kernel_size=3,stride=1,padding=1)\n",
    "    def forward(self, x):\n",
    "        x_seq = x[:,0:60*len(seq_fea_list)]\n",
    "        x_num = x[:,60*len(seq_fea_list):x.shape[1]]\n",
    "        x_seq = x_seq.reshape((-1,len(seq_fea_list),60))\n",
    "        x_seq = torch.transpose(x_seq, 1, 2)\n",
    "        x_num_repeat = x_num.reshape((-1,1,len(num_fea_list)))\n",
    "        x_num_repeat = x_num_repeat.repeat((1,60,1))\n",
    "        \n",
    "        x_seq = F.elu(self.Linear_1(torch.concat((x_seq,x_num_repeat),dim=-1)/5))\n",
    "        \n",
    "        enc_ftrs = self.encoder(torch.transpose(x_seq,1, 2))\n",
    "        x_seq_1 = self.decoder(enc_ftrs[::-1][0], enc_ftrs[::-1][1:])\n",
    "        x_seq_1 = torch.transpose(x_seq_1, 1, 2)\n",
    "        \n",
    "        # print(x_seq_1.shape)\n",
    "        \n",
    "        x_seq_1_mean = torch.mean(x_seq_1,dim=1,keepdim=True)\n",
    "        x_seq_1_mean = x_seq_1_mean.repeat((1,60,1))\n",
    "\n",
    "        x_seq_1_avg_pool = self.avg_pool_1(torch.transpose(x_seq_1, 1, 2))\n",
    "        x_seq_1_avg_pool = torch.transpose(x_seq_1_avg_pool,1, 2)\n",
    "        \n",
    "        x_seq_1 = F.elu(self.Linear_2(torch.cat((x_seq_1,x_seq_1_mean,x_seq,x_seq_1_avg_pool),dim=-1)/5))\n",
    "        \n",
    "        x_seq_out = self.Linear_3(x_seq_1)\n",
    "        x_seq_out = torch.transpose(x_seq_out, 1, 2)\n",
    "        x_seq_out = x_seq_out.reshape((-1,60*len(seq_y_list)))\n",
    "        \n",
    "        x_num_out = F.elu(self.Linear_4_0(torch.mean(x_seq_1,dim=1)))\n",
    "        x_num_out = self.Linear_4(x_num_out)\n",
    "        \n",
    "        \n",
    "        # print(self.weight)\n",
    "        \n",
    "        return self.weight.weight*(torch.concat((x_seq_out,x_num_out),dim=-1))/3+self.bias.weight/3\n",
    "    \n",
    "\n",
    "class FFNN_UNET_5(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(FFNN_UNET_5, self).__init__()\n",
    "        \n",
    "        self.encode_dim = 310\n",
    "        self.hidden_dim = 200\n",
    "        self.iter_dim = 320\n",
    "        \n",
    "\n",
    "        self.enc_channels= (self.encode_dim, self.encode_dim*2, self.encode_dim*4, self.encode_dim*8)\n",
    "        self.dec_channels= (self.encode_dim*8, self.encode_dim*4, self.encode_dim*2)\n",
    "\n",
    "        self.encoder = Encoder(self.enc_channels)\n",
    "        self.decoder = Decoder(self.dec_channels)\n",
    "        \n",
    "        # self.LSTM_1 = LSTM(self.encode_dim,self.hidden_dim,5,batch_first=True,dropout=0.05,bidirectional=True)\n",
    "        self.input_size = input_size\n",
    "        self.Linear_1 = nn.Linear(len(seq_fea_list)+len(num_fea_list), self.encode_dim)\n",
    "        self.Linear_2 = nn.Linear(6*self.encode_dim+self.encode_dim, self.iter_dim)\n",
    "        self.Linear_3 = nn.Linear(self.iter_dim, len(seq_y_list))\n",
    "        \n",
    "        self.Linear_4_0 = nn.Linear(self.iter_dim, self.iter_dim*2)\n",
    "\n",
    "        self.Linear_4 = nn.Linear(self.iter_dim*2, len(num_y_list))\n",
    "   \n",
    "        \n",
    "        \n",
    "        # self.Linear_4 = nn.Linear(len(seq_fea_list)*60+len(num_fea_list)+60*self.iter_dim+len(num_fea_list), 2*self.iter_dim)\n",
    "        self.Linear_5 = nn.Linear(2*self.iter_dim,len(num_y_list))\n",
    "        self.bias = nn.Linear(len(seq_y_list)*60+len(num_y_list),1)\n",
    "        self.weight = nn.Linear(len(seq_y_list)*60+len(num_y_list),1)\n",
    "        self.avg_pool_1 = AvgPool1d(kernel_size=3,stride=1,padding=1)\n",
    "    def forward(self, x):\n",
    "        x_seq = x[:,0:60*len(seq_fea_list)]\n",
    "        x_num = x[:,60*len(seq_fea_list):x.shape[1]]\n",
    "        x_seq = x_seq.reshape((-1,len(seq_fea_list),60))\n",
    "        x_seq = torch.transpose(x_seq, 1, 2)\n",
    "        x_num_repeat = x_num.reshape((-1,1,len(num_fea_list)))\n",
    "        x_num_repeat = x_num_repeat.repeat((1,60,1))\n",
    "        \n",
    "        x_seq = F.elu(self.Linear_1(torch.concat((x_seq,x_num_repeat),dim=-1)/5))\n",
    "        \n",
    "        enc_ftrs = self.encoder(torch.transpose(x_seq,1, 2))\n",
    "        x_seq_1 = self.decoder(enc_ftrs[::-1][0], enc_ftrs[::-1][1:])\n",
    "        x_seq_1 = torch.transpose(x_seq_1, 1, 2)\n",
    "        \n",
    "        # print(x_seq_1.shape)\n",
    "        \n",
    "        x_seq_1_mean = torch.mean(x_seq_1,dim=1,keepdim=True)\n",
    "        x_seq_1_mean = x_seq_1_mean.repeat((1,60,1))\n",
    "\n",
    "        x_seq_1_avg_pool = self.avg_pool_1(torch.transpose(x_seq_1, 1, 2))\n",
    "        x_seq_1_avg_pool = torch.transpose(x_seq_1_avg_pool,1, 2)\n",
    "        \n",
    "        x_seq_1 = F.elu(self.Linear_2(torch.cat((x_seq_1,x_seq_1_mean,x_seq,x_seq_1_avg_pool),dim=-1)/5))\n",
    "        \n",
    "        x_seq_out = self.Linear_3(x_seq_1)\n",
    "        x_seq_out = torch.transpose(x_seq_out, 1, 2)\n",
    "        x_seq_out = x_seq_out.reshape((-1,60*len(seq_y_list)))\n",
    "        \n",
    "        x_num_out = F.elu(self.Linear_4_0(torch.mean(x_seq_1,dim=1)))\n",
    "        x_num_out = self.Linear_4(x_num_out)\n",
    "        \n",
    "        \n",
    "        # print(self.weight)\n",
    "        \n",
    "        return self.weight.weight*(torch.concat((x_seq_out,x_num_out),dim=-1))/3+self.bias.weight/3\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class FFNN_UNET_6(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(FFNN_UNET_6, self).__init__()\n",
    "        \n",
    "        self.encode_dim = 200\n",
    "        self.hidden_dim = 160\n",
    "        self.iter_dim = 500\n",
    "        \n",
    "\n",
    "        self.enc_channels= (self.encode_dim, self.encode_dim*2, self.encode_dim*4, self.encode_dim*8)\n",
    "        self.dec_channels= (self.encode_dim*8, self.encode_dim*4, self.encode_dim*2)\n",
    "\n",
    "        self.encoder = Encoder(self.enc_channels)\n",
    "        self.decoder = Decoder(self.dec_channels)\n",
    "        \n",
    "        # self.LSTM_1 = LSTM(self.encode_dim,self.hidden_dim,5,batch_first=True,dropout=0.05,bidirectional=True)\n",
    "        self.input_size = input_size\n",
    "        self.Linear_1 = nn.Linear(len(seq_fea_list)+len(num_fea_list), self.encode_dim)\n",
    "        self.Linear_2 = nn.Linear(6*self.encode_dim+self.encode_dim, self.iter_dim)\n",
    "        self.Linear_3 = nn.Linear(self.iter_dim, len(seq_y_list))\n",
    "        \n",
    "        self.Linear_4_0 = nn.Linear(self.iter_dim, self.iter_dim*2)\n",
    "\n",
    "        self.Linear_4 = nn.Linear(self.iter_dim*2, len(num_y_list))\n",
    "   \n",
    "        \n",
    "        \n",
    "        # self.Linear_4 = nn.Linear(len(seq_fea_list)*60+len(num_fea_list)+60*self.iter_dim+len(num_fea_list), 2*self.iter_dim)\n",
    "        self.Linear_5 = nn.Linear(2*self.iter_dim,len(num_y_list))\n",
    "        self.bias = nn.Linear(len(seq_y_list)*60+len(num_y_list),1)\n",
    "        self.weight = nn.Linear(len(seq_y_list)*60+len(num_y_list),1)\n",
    "        self.avg_pool_1 = AvgPool1d(kernel_size=3,stride=1,padding=1)\n",
    "    def forward(self, x):\n",
    "        x_seq = x[:,0:60*len(seq_fea_list)]\n",
    "        x_num = x[:,60*len(seq_fea_list):x.shape[1]]\n",
    "        x_seq = x_seq.reshape((-1,len(seq_fea_list),60))\n",
    "        x_seq = torch.transpose(x_seq, 1, 2)\n",
    "        x_num_repeat = x_num.reshape((-1,1,len(num_fea_list)))\n",
    "        x_num_repeat = x_num_repeat.repeat((1,60,1))\n",
    "        \n",
    "        x_seq = F.elu(self.Linear_1(torch.concat((x_seq,x_num_repeat),dim=-1)/5))\n",
    "        \n",
    "        enc_ftrs = self.encoder(torch.transpose(x_seq,1, 2))\n",
    "        x_seq_1 = self.decoder(enc_ftrs[::-1][0], enc_ftrs[::-1][1:])\n",
    "        x_seq_1 = torch.transpose(x_seq_1, 1, 2)\n",
    "        \n",
    "        # print(x_seq_1.shape)\n",
    "        \n",
    "        x_seq_1_mean = torch.mean(x_seq_1,dim=1,keepdim=True)\n",
    "        x_seq_1_mean = x_seq_1_mean.repeat((1,60,1))\n",
    "\n",
    "        x_seq_1_avg_pool = self.avg_pool_1(torch.transpose(x_seq_1, 1, 2))\n",
    "        x_seq_1_avg_pool = torch.transpose(x_seq_1_avg_pool,1, 2)\n",
    "        \n",
    "        x_seq_1 = F.elu(self.Linear_2(torch.cat((x_seq_1,x_seq_1_mean,x_seq,x_seq_1_avg_pool),dim=-1)/5))\n",
    "        \n",
    "        x_seq_out = self.Linear_3(x_seq_1)\n",
    "        x_seq_out = torch.transpose(x_seq_out, 1, 2)\n",
    "        x_seq_out = x_seq_out.reshape((-1,60*len(seq_y_list)))\n",
    "        \n",
    "        x_num_out = F.elu(self.Linear_4_0(torch.mean(x_seq_1,dim=1)))\n",
    "        x_num_out = self.Linear_4(x_num_out)\n",
    "        \n",
    "        \n",
    "        # print(self.weight)\n",
    "        \n",
    "        return self.weight.weight*(torch.concat((x_seq_out,x_num_out),dim=-1))/3+self.bias.weight/3\n",
    "    \n",
    "\n",
    "input_size = x_valid.shape[1]\n",
    "output_size = y_valid.shape[1]\n",
    "hidden_size = input_size + output_size\n",
    "\n",
    "model_single = FFNN_LSTM_999_AVG_ATT(input_size, output_size)\n",
    "device_ids = list(range(torch.cuda.device_count()))\n",
    "model = torch.nn.DataParallel(model_single)\n",
    "zeroout_index.to(device)\n",
    "\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "#model = FFNN_UNET_3(input_size, output_size).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "criterion_l1 = nn.L1Loss()\n",
    "criterion_huber = nn.HuberLoss(delta=0.5)\n",
    "\n",
    "#model.load_state_dict(torch.load(DATA_PATH+'FFNN_LSTM_888_AVG_ATT_1e-15_12.pt'))\n",
    "\n",
    "\n",
    "from ema_pytorch import EMA\n",
    "ema = EMA(\n",
    "    model,\n",
    "    beta = 0.99,              # exponential moving average factor\n",
    "    update_after_step = 50,    # only after this number of .update() calls will it start updating\n",
    "    update_every = 8,          # how often to actually update, to save on compute (updates every 10th .update() call)\n",
    ")\n",
    "# model.load_state_dict(torch.load(DATA_PATH+'model.pt'))\n",
    "\n",
    "\n",
    "# ema = hkl.load(DATA_PATH+'ema.hkl')\n",
    "\n",
    "\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.0002)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=SCHEDULER_FACTOR, patience=SCHEDULER_PATIENCE)\n",
    "\n",
    "print(\"Time after all preparations:\", format_time(time.time()-ts), flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5c22d3-9e84-47c2-b068-923ff56850b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Epoch: 2   Train Loss: 0.5134   LR: 1.0e-03   Time: 0:02:45\n",
      "  Epoch: 2   Train Loss: 0.5003   LR: 1.0e-03   Time: 0:05:16\n",
      "  Epoch: 2   Train Loss: 0.4746   LR: 1.0e-03   Time: 0:07:44\n",
      "  Epoch: 2   Train Loss: 0.4365   LR: 1.0e-03   Time: 0:10:15\n",
      "  Epoch: 2   Train Loss: 0.4026   LR: 1.0e-03   Time: 0:12:42\n",
      "  Epoch: 2   Train Loss: 0.3955   LR: 1.0e-03   Time: 0:15:12\n",
      "  Epoch: 2   Train Loss: 0.3608   LR: 1.0e-03   Time: 0:17:40\n",
      "  Epoch: 2   Train Loss: 0.3438   LR: 1.0e-03   Time: 0:20:11\n",
      "  Epoch: 2   Train Loss: 0.3138   LR: 1.0e-03   Time: 0:22:58\n",
      "  Epoch: 2   Train Loss: 0.3123   LR: 1.0e-03   Time: 0:25:42\n",
      "  Epoch: 2   Train Loss: 0.3019   LR: 1.0e-03   Time: 0:28:28\n",
      "  Epoch: 2   Train Loss: 0.3076   LR: 1.0e-03   Time: 0:31:16\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "time_gap = 0.00000005\n",
    "# for g in optimizer.param_groups:\n",
    "#     g['lr'] = g['lr']/2\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_model_state = None\n",
    "patience_count = 0\n",
    "r2score = R2Score(num_outputs=y_valid.shape[1]).to(device)\n",
    "for epoch in range(1,400):\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    random.shuffle(file_list)\n",
    "    random.shuffle(file_list_mix)\n",
    "\n",
    "    #g['lr'] \n",
    "    \n",
    "    if epoch > 9:\n",
    "        for g in optimizer.param_groups:\n",
    "            #g['lr'] = max(g['lr']*0.8,1e-4)\n",
    "            g['lr']  = 1e-4\n",
    "\n",
    "\n",
    "    print(\"\")\n",
    "    model.train()\n",
    "    ema.train()\n",
    "    total_loss = 0\n",
    "    steps = 0\n",
    "    batch_idx = -1\n",
    "    \n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    batch_num = 105\n",
    "\n",
    "    x_train = np.zeros((batch_num*384*6,input_size),dtype=np.float32)\n",
    "    y_train = np.zeros((batch_num*384*6,output_size),dtype=np.float32)\n",
    "\n",
    "    x_train_mix = np.zeros((batch_num*384*6,input_size),dtype=np.float32)\n",
    "    y_train_mix = np.zeros((batch_num*384*6,output_size),dtype=np.float32)\n",
    "\n",
    "    for file_i in range(len(file_list)): \n",
    "        \n",
    "        y_tmp = hkl.load(file_list[file_i][0:30]+'y'+file_list[file_i][31:len(file_list[file_i])])\n",
    "        x_tmp = hkl.load(file_list[file_i])\n",
    "\n",
    "        y_tmp_mix = hkl.load(file_list_mix[file_i][0:32]+'y'+file_list_mix[file_i][33:len(file_list_mix[file_i])])\n",
    "        x_tmp_mix = hkl.load(file_list_mix[file_i])\n",
    "        #print(file_i%246*384*6,file_i%246*384*6+384*6)\n",
    "        x_train[file_i%batch_num*384*6:file_i%batch_num*384*6+384*6,:] = x_tmp\n",
    "        y_train[file_i%batch_num*384*6:file_i%batch_num*384*6+384*6,:] = y_tmp\n",
    "\n",
    "        x_train_mix[file_i%batch_num*384*6:file_i%batch_num*384*6+384*6,:] = x_tmp_mix\n",
    "        y_train_mix[file_i%batch_num*384*6:file_i%batch_num*384*6+384*6,:] = y_tmp_mix\n",
    "\n",
    "        #y_train[:,zeroout_index] = y_train[:,zeroout_index]*0.0\n",
    "        \n",
    "        if (file_i+1)%batch_num == 0:\n",
    "            gc.collect()\n",
    "\n",
    "\n",
    "            y_train[:,zeroout_index] = y_train[:,zeroout_index]*0.0\n",
    "            y_train_mix[:,zeroout_index] = y_train_mix[:,zeroout_index]*0.0\n",
    "          \n",
    "            random_index = np.random.permutation(x_train.shape[0])\n",
    "            i1 = 0\n",
    "            for i in range(x_train.shape[0]//BATCH_SIZE+1):\n",
    "                time.sleep(time_gap)\n",
    "                i2 = np.minimum(i1 + BATCH_SIZE, x_train.shape[0])\n",
    "                if i1 == i2:  # Break the loop if range does not change\n",
    "                    break\n",
    "        \n",
    "                # Convert the current slice of xt to a PyTorch tensor\n",
    "                inputs = torch.from_numpy(x_train[random_index[i1:i2], :]).to(device)\n",
    "                batch_idx = batch_idx + 1\n",
    "                # if batch_idx%n_rounds == 0:\n",
    "                #     optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                outputs_y = torch.from_numpy(y_train[random_index[i1:i2], :]).to(device)\n",
    "\n",
    "                inputs_mix = torch.from_numpy(x_train_mix[random_index[i1:i2], :]).to(device)\n",
    "                # if batch_idx%n_rounds == 0:\n",
    "                #     optimizer.zero_grad()\n",
    "                outputs_mix = model(inputs_mix)\n",
    "                outputs_y_mix = torch.from_numpy(y_train_mix[random_index[i1:i2], :]).to(device)\n",
    "\n",
    "                \n",
    "                loss = 0.08*criterion(outputs,outputs_y)/n_rounds+criterion_l1(outputs,outputs_y)/n_rounds\n",
    "                loss = loss + 0.3*criterion_l1(outputs_mix,outputs_y_mix)/n_rounds\n",
    "\n",
    "                #loss = loss + 0.0001*criterion(outputs_mix,outputs_y_mix)/n_rounds\n",
    "                loss = loss + 0.0005*criterion(outputs_mix,outputs_y_mix)/n_rounds\n",
    "\n",
    "                #loss = torch.mean(loss*loss,axis=0)/error_mean\n",
    "                #loss = torch.mean(loss)\n",
    "                loss.backward()\n",
    "                #torch.nn.utils.clip_grad_norm_(model.parameters(), 1e-3)\n",
    "\n",
    "                if batch_idx%n_rounds == 0:\n",
    "                    # loss.backward()\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    optimizer.step()\n",
    "                    #torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    ema.update()\n",
    "                # print(model.weight.weight[0:10])\n",
    "        \n",
    "                total_loss += loss.item()\n",
    "                steps += 1\n",
    "        \n",
    "                if (batch_idx + 1) % PRINT_FREQ == 0:\n",
    "                    current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "                    elapsed_time = format_time(time.time() - ts)\n",
    "                    print(f'  Epoch: {epoch+1}',\\\n",
    "                          # f'  Batch: {batch_idx + 1}',\\\n",
    "                          f'  Train Loss: {total_loss / steps:.4f}',\\\n",
    "                          f'  LR: {current_lr:.1e}',\\\n",
    "                          f'  Time: {elapsed_time}', flush=True)\n",
    "                    with open('log0.txt', 'a') as file:\n",
    "                        file.write(f'  Epoch: {epoch+1}  Train Loss: {total_loss / steps:.4f}  LR: {current_lr:.1e} Time: {elapsed_time}' + '\\n')\n",
    "                    total_loss = 0\n",
    "                    steps = 0\n",
    "        \n",
    "                # No need to track gradients for inference\n",
    "        \n",
    "                i1 = i2  # Update i1 to the end of the current batch\n",
    "        \n",
    "                if i2 >= x_train.shape[0]:\n",
    "                    break\n",
    "            #x_train = np.zeros((batch_num*384*6,input_size)),dtype=np.float32)\n",
    "            #y_train = np.zeros((batch_num*384*6,output_size)),dtype=np.float32)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    y_true = torch.tensor([], device=device)\n",
    "    all_outputs = torch.tensor([], device=device)\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(val_loader):\n",
    "            time.sleep(time_gap)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            val_loss += criterion(outputs, labels).item()\n",
    "            y_true = torch.cat((y_true, labels), 0)\n",
    "            all_outputs = torch.cat((all_outputs, outputs), 0)\n",
    "    r2=0\n",
    "    r2_broken = []\n",
    "    r2_broken_names = []\n",
    "    for i in range(368):\n",
    "        r2_i = r2score(all_outputs[:, i], y_true[:, i])\n",
    "        if r2_i > 1e-6:\n",
    "            r2 += r2_i\n",
    "        else:\n",
    "            r2_broken.append(i)\n",
    "            r2_broken_names.append(FEAT_COLS[i])\n",
    "    r2 /= 368\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f'\\nEpoch: {epoch+1}  Val Loss: {avg_val_loss:.4f}  R2 score: {r2:.4f}')\n",
    "    print(f'{len(r2_broken)} targets were excluded during evaluation of R2 score.')\n",
    "    with open('log0.txt', 'a') as file:\n",
    "            file.write(f'\\nEpoch: {epoch+1}  Val Loss: {avg_val_loss:.4f}  R2 score: {r2:.4f}' + '\\n')\n",
    "    # print(r2_broken)\n",
    "    # print(r2_broken_names, flush=True)\n",
    "    scheduler.step(round(avg_val_loss*2,4))\n",
    "    \n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        best_model_state = model.state_dict()\n",
    "        patience_count = 0\n",
    "        print(\"Validation loss decreased, saving new best model and resetting patience counter.\")\n",
    "    else:\n",
    "        patience_count += 1\n",
    "        print(f\"No improvement in validation loss for {patience_count} epochs.\")\n",
    "        \n",
    "    if patience_count >= PATIENCE:\n",
    "        print(\"Stopping early due to no improvement in validation loss.\")\n",
    "        break\n",
    "    \n",
    "    ema.eval()\n",
    "    val_loss = 0\n",
    "    y_true = torch.tensor([], device=device)\n",
    "    all_outputs = torch.tensor([], device=device)\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(val_loader):\n",
    "            time.sleep(time_gap)\n",
    "\n",
    "            outputs = ema(inputs)\n",
    "            val_loss += criterion(outputs, labels).item()\n",
    "            y_true = torch.cat((y_true, labels), 0)\n",
    "            all_outputs = torch.cat((all_outputs, outputs), 0)\n",
    "    valid_results = all_outputs.to('cpu').numpy()\n",
    "    #hkl.dump(valid_results, DATA_PATH+'valid_full_FFNN_LSTM_6_AVG_2_'+str(epoch)+'.hkl', compression='gzip')\n",
    "    r2=0\n",
    "    r2_broken = []\n",
    "    r2_broken_names = []\n",
    "    for i in range(368):\n",
    "        r2_i = r2score(all_outputs[:, i], y_true[:, i])\n",
    "        if r2_i > 1e-6:\n",
    "            r2 += r2_i\n",
    "        else:\n",
    "            r2_broken.append(i)\n",
    "            r2_broken_names.append(FEAT_COLS[i])\n",
    "    r2 /= 368\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f'\\nEpoch: {epoch+1}  Val Loss: {avg_val_loss:.4f}  R2 score: {r2:.4f}')\n",
    "    with open('log0.txt', 'a') as file:\n",
    "        file.write(f'\\nEpoch: {epoch+1}  Val Loss: {avg_val_loss:.4f}  R2 score: {r2:.4f}' + '\\n')\n",
    "    print(f'{len(r2_broken)} targets were excluded during evaluation of R2 score.')\n",
    "    # print(r2_broken)\n",
    "    # print(r2_broken_names, flush=True)\n",
    "    predt = np.zeros([x_test.shape[0], output_size], dtype=np.float32)\n",
    "    # b = np.concatenate((a, a, a),axis=0)\n",
    "    if epoch%1==0 and epoch>0:\n",
    "        i1 = 0\n",
    "        for i in tqdm(range(10000)):\n",
    "            time.sleep(time_gap)\n",
    "\n",
    "            i2 = np.minimum(i1 + BATCH_SIZE, x_test.shape[0])\n",
    "            if i1 == i2:  # Break the loop if range does not change\n",
    "                break\n",
    "    \n",
    "            # Convert the current slice of xt to a PyTorch tensor\n",
    "            inputs = torch.from_numpy(x_test[i1:i2, :]).float().to(device)\n",
    "    \n",
    "            # No need to track gradients for inference\n",
    "            with torch.no_grad():\n",
    "                outputs = ema(inputs)  # Get model predictions\n",
    "                predt[i1:i2, :] = outputs.cpu().numpy()  # Store predictions in predt\n",
    "    \n",
    "            i1 = i2  # Update i1 to the end of the current batch\n",
    "    \n",
    "            if i2 >= x_test.shape[0]:\n",
    "                break\n",
    "        hkl.dump(predt, DATA_PATH+'FFNN_LSTM_999_AVG_ATT_'+str(epoch)+'_mix_1e-15.hkl', compression='gzip')\n",
    "        torch.save(model.state_dict(), DATA_PATH+'FFNN_LSTM_999_AVG_ATT_1e-15_'+str(epoch)+'.pt')\n",
    "        torch.save(ema.state_dict(), DATA_PATH+'FFNN_LSTM_999_AVG_ATT_1e-15_ema_'+str(epoch)+'.pt')\n",
    "\n",
    "        # hkl.dump(ema,DATA_PATH+'ema.hkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3745204e-6b1f-487b-a62d-5f61e8e7104f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len('autodl-tmp/CD Fragment/c_data_frag_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f18868e-c0a1-4f60-a0a8-e174a093f73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(ema.state_dict(), DATA_PATH+'FFNN_LSTM_749_AVG_ATT_1e-15_ema_'+str(epoch)+'.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4b11be-e0b0-435b-9a75-80a53dcbd68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), DATA_PATH+'FFNN_LSTM_7_AVG_ATT_1e-15_'+str(epoch)+'.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf30fecd-b2ef-48c9-beb3-bf74cb83155b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ema.eval()\n",
    "val_loss = 0\n",
    "y_true = torch.tensor([], device=device)\n",
    "all_outputs = torch.tensor([], device=device)\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in tqdm(val_loader):\n",
    "        time.sleep(time_gap)\n",
    "\n",
    "        outputs = ema(inputs)\n",
    "        val_loss += criterion(outputs, labels).item()\n",
    "        y_true = torch.cat((y_true, labels), 0)\n",
    "        all_outputs = torch.cat((all_outputs, outputs), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ef3eb5-dd3f-4df2-a534-079de92dafcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31bcabb-80d5-453f-8131-6402d12064a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_num = 2000\n",
    "error_mean = 0\n",
    "for file_i in tqdm(range(error_num)):\n",
    "    y_tmp = hkl.load(file_list[file_i][0:35]+'y'+file_list[file_i][36:len(file_list[file_i])])\n",
    "    x_tmp = hkl.load(file_list[file_i])\n",
    "    y_tmp = torch.from_numpy(y_tmp).to(device)\n",
    "    inputs = torch.from_numpy(x_tmp).to(device)\n",
    "    error = ema(inputs)-y_tmp\n",
    "    error_mean = error_mean + torch.mean(error*error,dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1698b3d-1537-4332-baf5-9c36d99f76c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_mean = error_mean.to('cpu').numpy()/error_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014e7599-5dc2-4ace-bc40-51a4e3236dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "(error_mean+1e-2)*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9327126-b3c9-4650-a828-fbdba3e28bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(error_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c11d66b-7a5c-46cd-a121-14425a9c57ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "hkl.dump(error_mean,'error_mean.hkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ec14fd-effd-4b6e-9649-b68ce1256820",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(368):\n",
    "    error[i] = np.abs(tmp[:,i] - y_valid[:,i]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609d2eaf-3179-4621-837b-02daf21173a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mean(error*error,dim=0)+0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0adc874-1a1b-4aad-9804-0a9e3f219abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sort(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7c340e-015a-4b2d-8458-4036498a2f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "error[120:147]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fff1bd5-c4d6-411b-a5c4-9c06f8e27d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "y[:,120:147]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776cfc78-1556-4ef2-a7cc-32d805bde19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid[:,120:147]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4138b616-1c50-4e85-bbd3-432ca657eb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(368):\n",
    "    tmp_i = tmp[:,i] - y_valid[:,i]\n",
    "    tmp_i[tmp_i>0]=1\n",
    "    tmp_i[tmp_i<0]=-1\n",
    "    error[i] = abs(tmp_i.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8572a3a0-f411-43c6-a679-c23e69b1a004",
   "metadata": {},
   "outputs": [],
   "source": [
    "ema.load_state_dict(torch.load(DATA_PATH+'FFNN_LSTM_749_AVG_ATT_1e-15_ema_30.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a061bf-a492-4b1d-a268-022e80bbaf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predt = np.zeros([x_test.shape[0], output_size], dtype=np.float32)\n",
    "# b = np.concatenate((a, a, a),axis=0)\n",
    "\n",
    "i1 = 0\n",
    "for i in tqdm(range(10000)):\n",
    "\n",
    "    i2 = np.minimum(i1 + BATCH_SIZE, x_test.shape[0])\n",
    "    if i1 == i2:  # Break the loop if range does not change\n",
    "        break\n",
    "\n",
    "    # Convert the current slice of xt to a PyTorch tensor\n",
    "    inputs = torch.from_numpy(x_test[i1:i2, :]).float().to(device)\n",
    "\n",
    "    # No need to track gradients for inference\n",
    "    with torch.no_grad():\n",
    "        outputs = ema(inputs)  # Get model predictions\n",
    "        predt[i1:i2, :] = outputs.cpu().numpy()  # Store predictions in predt\n",
    "\n",
    "    i1 = i2  # Update i1 to the end of the current batch\n",
    "\n",
    "    if i2 >= x_test.shape[0]:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2ac585-98d9-43b1-bda4-1e4f2bafc246",
   "metadata": {},
   "outputs": [],
   "source": [
    "zeroout_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3263b06f-7d80-429e-abca-01694be28f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05938f3-b883-40f6-8086-ca150f448376",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92401f3-b070-42d0-ba31-877cdbe0e62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4c4aee-72f9-4f78-8578-b52f54694f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b426700-0434-467e-b844-7b6522079036",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "val_loss = 0\n",
    "y_true = torch.tensor([], device=device)\n",
    "all_outputs = torch.tensor([], device=device)\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in tqdm(val_loader):\n",
    "        time.sleep(time_gap)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        val_loss += criterion(outputs, labels).item()\n",
    "        y_true = torch.cat((y_true, labels), 0)\n",
    "        all_outputs = torch.cat((all_outputs, outputs), 0)\n",
    "r2=0\n",
    "r2_broken = []\n",
    "r2_broken_names = []\n",
    "for i in range(368):\n",
    "    r2_i = r2score(all_outputs[:, i], y_true[:, i])\n",
    "    if r2_i > 1e-6:\n",
    "        r2 += r2_i\n",
    "    else:\n",
    "        r2_broken.append(i)\n",
    "        r2_broken_names.append(FEAT_COLS[i])\n",
    "r2 /= 368\n",
    "\n",
    "avg_val_loss = val_loss / len(val_loader)\n",
    "print(f'\\nEpoch: {epoch+1}  Val Loss: {avg_val_loss:.4f}  R2 score: {r2:.4f}')\n",
    "print(f'{len(r2_broken)} targets were excluded during evaluation of R2 score.')\n",
    "with open('log0.txt', 'a') as file:\n",
    "        file.write(f'\\nEpoch: {epoch+1}  Val Loss: {avg_val_loss:.4f}  R2 score: {r2:.4f}' + '\\n')\n",
    "# print(r2_broken)\n",
    "# print(r2_broken_names, flush=True)\n",
    "scheduler.step(round(avg_val_loss*2,4))\n",
    "\n",
    "if avg_val_loss < best_val_loss:\n",
    "    best_val_loss = avg_val_loss\n",
    "    best_model_state = model.state_dict()\n",
    "    patience_count = 0\n",
    "    print(\"Validation loss decreased, saving new best model and resetting patience counter.\")\n",
    "else:\n",
    "    patience_count += 1\n",
    "    print(f\"No improvement in validation loss for {patience_count} epochs.\")\n",
    "    \n",
    "if patience_count >= PATIENCE:\n",
    "    print(\"Stopping early due to no improvement in validation loss.\")\n",
    "    break\n",
    "\n",
    "ema.eval()\n",
    "val_loss = 0\n",
    "y_true = torch.tensor([], device=device)\n",
    "all_outputs = torch.tensor([], device=device)\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in tqdm(val_loader):\n",
    "        time.sleep(time_gap)\n",
    "\n",
    "        outputs = ema(inputs)\n",
    "        val_loss += criterion(outputs, labels).item()\n",
    "        y_true = torch.cat((y_true, labels), 0)\n",
    "        all_outputs = torch.cat((all_outputs, outputs), 0)\n",
    "valid_results = all_outputs.to('cpu').numpy()\n",
    "#hkl.dump(valid_results, DATA_PATH+'valid_full_FFNN_LSTM_6_AVG_2_'+str(epoch)+'.hkl', compression='gzip')\n",
    "r2=0\n",
    "r2_broken = []\n",
    "r2_broken_names = []\n",
    "for i in range(368):\n",
    "    r2_i = r2score(all_outputs[:, i], y_true[:, i])\n",
    "    if r2_i > 1e-6:\n",
    "        r2 += r2_i\n",
    "    else:\n",
    "        r2_broken.append(i)\n",
    "        r2_broken_names.append(FEAT_COLS[i])\n",
    "r2 /= 368\n",
    "\n",
    "avg_val_loss = val_loss / len(val_loader)\n",
    "print(f'\\nEpoch: {epoch+1}  Val Loss: {avg_val_loss:.4f}  R2 score: {r2:.4f}')\n",
    "with open('log0.txt', 'a') as file:\n",
    "    file.write(f'\\nEpoch: {epoch+1}  Val Loss: {avg_val_loss:.4f}  R2 score: {r2:.4f}' + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d62052-8794-4e11-97bc-aada7d45182a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ema.eval()\n",
    "val_loss = 0\n",
    "y_true = torch.tensor([], device=device)\n",
    "all_outputs = torch.tensor([], device=device)\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in tqdm(val_loader):\n",
    "        time.sleep(time_gap)\n",
    "\n",
    "        outputs = ema(inputs)\n",
    "        val_loss += criterion(outputs, labels).item()\n",
    "        y_true = torch.cat((y_true, labels), 0)\n",
    "        all_outputs = torch.cat((all_outputs, outputs), 0)\n",
    "valid_results = all_outputs.to('cpu').numpy()\n",
    "#hkl.dump(valid_results, DATA_PATH+'valid_full_FFNN_LSTM_6_AVG_2_'+str(epoch)+'.hkl', compression='gzip')\n",
    "r2=0\n",
    "r2_broken = []\n",
    "r2_broken_names = []\n",
    "for i in range(368):\n",
    "    r2_i = r2score(all_outputs[:, i], y_true[:, i])\n",
    "    if r2_i > 1e-6:\n",
    "        r2 += r2_i\n",
    "    else:\n",
    "        r2_broken.append(i)\n",
    "        r2_broken_names.append(FEAT_COLS[i])\n",
    "r2 /= 368\n",
    "\n",
    "avg_val_loss = val_loss / len(val_loader)\n",
    "print(f'\\nEpoch: {epoch+1}  Val Loss: {avg_val_loss:.4f}  R2 score: {r2:.4f}')\n",
    "with open('log0.txt', 'a') as file:\n",
    "    file.write(f'\\nEpoch: {epoch+1}  Val Loss: {avg_val_loss:.4f}  R2 score: {r2:.4f}' + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920cea6d-2c2a-4d48-a4df-2b1444172f4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
